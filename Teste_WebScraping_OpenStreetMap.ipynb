{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das bibliotecas necessárias para a raspagem de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "# pip install webdriver-manager\n",
    "# pip install BeautifulSoup4\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "servico = Service(ChromeDriverManager().install())\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# import requests\n",
    "# options = Options()\n",
    "# options.add_argument('--headless') #--> Parâmetro adicional onde é possivel realizar o webscrapping sem abrir o navegador.\n",
    "# options.add_argument('window-size=400,800') #--> Parâmetro adicional onde é possivel escolher o tamanho da tela aberta no navegador. Nesse exemplo o tamanho 400x800 é como se estivesse aberto em um celular.\n",
    "\n",
    "URL_OPEN_STREET_MAP_TRACES = 'https://www.openstreetmap.org/traces' #--> Página do OpenstreetMap onde estão localizadas as rotas para download.\n",
    "PREFIX_URL_DOWNLOAD = 'https://www.openstreetmap.org' #--> Página do principal do OpenstreetMap. Esta variável será utilizada para montar a URL das páginas de download.\n",
    "DOWNLOADS = '/home/thiago/Downloads/' # Caminho para a pasta de downloads.\n",
    "PRE_PROCESSAMENTO = '/home/thiago/tcc_ufrj/PRE_PROCESSAMENTO' # Caminho para a pasta de pré-processamento dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessando o site Open Street Map e capturando as rotas pendentes e as rotas finalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# navegador = webdriver.Chrome(service=servico, options=options) #--> Aplicando as opções acima mencionadas no navegador.\n",
    "\n",
    "navegador = webdriver.Chrome(service=servico)\n",
    "navegador.get(URL_OPEN_STREET_MAP_TRACES)\n",
    "\n",
    "conteudo_da_pagina = navegador.page_source\n",
    "site = BeautifulSoup(conteudo_da_pagina, 'html.parser')\n",
    "routes = site.findAll('tr')\n",
    "\n",
    "list_rotas_pendentes = []\n",
    "list_rotas_finalizadas = []\n",
    "lista_rotas = []\n",
    "\n",
    "for route in routes:\n",
    "    if route.find('span', attrs={'class': 'text-danger'}):\n",
    "        rotas_pendentes = route.find('span', attrs={'class': 'text-danger'})    \n",
    "        link_rotas_pendentes = route.find('a')        \n",
    "        lista_rotas.append([PREFIX_URL_DOWNLOAD+link_rotas_pendentes['href']])\n",
    "        # list_rotas_pendentes.append([PREFIX_URL_DOWNLOAD+link_rotas_pendentes['href']])\n",
    "        print (f\"{rotas_pendentes.text}: {PREFIX_URL_DOWNLOAD+link_rotas_pendentes['href']}\")\n",
    "\n",
    "    else:         \n",
    "        link_rotas_finalizadas = route.find('a')        \n",
    "        lista_rotas.append([PREFIX_URL_DOWNLOAD+link_rotas_finalizadas['href']])\n",
    "        # list_rotas_finalizadas.append([PREFIX_URL_DOWNLOAD+link_rotas_finalizadas['href']])\n",
    "        print(f\"FINISHED: {PREFIX_URL_DOWNLOAD+link_rotas_finalizadas['href']}\")\n",
    "time.sleep(3)\n",
    "navegador.close()\n",
    "\n",
    "#len(list_rotas_pendentes) #--> Verificando a quantidade de registros na lista de rotas pendentes\n",
    "#len(list_rotas_finalizadas) #--> Verificando a quantidade de registros na lista de rotas pendentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acessando a telas para efetuar os downloads - Renomeando os arquivo adicionando o nome dos usuários - Movendo para a pasta PRE_PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navegador = webdriver.Chrome(service=servico)\n",
    "users = []\n",
    "for list_route in lista_rotas:\n",
    "    time.sleep(3)\n",
    "    url = list_route[0]\n",
    "    navegador.get(url) #--> Exemplo onde usamos o Selenium somente com o [.get]\n",
    "\n",
    "    user_conteudo_da_pagina = navegador.page_source\n",
    "    site_user = BeautifulSoup(user_conteudo_da_pagina, 'html.parser')\n",
    "\n",
    "    if any(td.find('span', attrs={'class': 'text-danger'}) for td in site_user):\n",
    "        tb_nome_usuario = navegador.find_element('xpath', '//*[@id=\"content\"]/div[2]/div/table/tbody/tr[4]/td')\n",
    "        nome_usuario = tb_nome_usuario.text\n",
    "        nome_usuario = re.sub(r'\\s|\\.|\\(|\\)','_',nome_usuario)        \n",
    "        print(f'Rota PENDENTE: {list_route} Usuário: {nome_usuario}')\n",
    "        navegador.find_element('xpath','//*[@id=\"content\"]/div[2]/div/table/tbody/tr[1]/td/a').click()\n",
    "        users.append(nome_usuario)\n",
    "\n",
    "    else:\n",
    "        tb_nome_usuario = navegador.find_element('xpath', '//*[@id=\"content\"]/div[2]/div/table/tbody/tr[6]/td')\n",
    "        nome_usuario = tb_nome_usuario.text\n",
    "        nome_usuario = re.sub(r'\\s|\\.|\\(|\\)','_',nome_usuario)\n",
    "        print(f'Rota FINALIZADA: {list_route} Usuário: {nome_usuario}')\n",
    "        navegador.find_element('xpath','//*[@id=\"content\"]/div[2]/div/table/tbody/tr[1]/td/a').click()\n",
    "        users.append(nome_usuario)\n",
    "time.sleep(3)\n",
    "navegador.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_rename = [arquivo for arquivo in os.listdir(DOWNLOADS) if arquivo.endswith(\".gz\")]\n",
    "for file_to_rename in files_to_rename:\n",
    "    diretorio_origem = os.path.join(DOWNLOADS, file_to_rename)\n",
    "    diretorio_destino = os.path.dirname(diretorio_origem)\n",
    "    nome_arquivo_extraido = file_to_rename[:-3]  # Remover a extensão .gz\n",
    "    with gzip.open(diretorio_origem, 'rb') as arquivo_gz, open(os.path.join(diretorio_destino, nome_arquivo_extraido), 'wb') as arquivo_extraido:\n",
    "        arquivo_extraido.write(arquivo_gz.read())\n",
    "    os.remove(diretorio_origem)   \n",
    "    \n",
    "\n",
    "\n",
    "    #diretorio_destino = os.path.dirname(caminho_arquivo_gz)\n",
    "    #with gzip.open(files_to_rename, 'rt') as arquivo:\n",
    "    #    conteudo = arquivo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrigindo o nome dos arquivos conforme seus usuários\n",
    "files_to_rename = [arquivo for arquivo in os.listdir(DOWNLOADS) if arquivo.endswith(\".crdownload\")]\n",
    "for file_name in files_to_rename:\n",
    "        novo_nome = file_name.replace(\".crdownload\", \"\")\n",
    "        os.rename(os.path.join(DOWNLOADS, file_name), os.path.join(DOWNLOADS, novo_nome))\n",
    "\n",
    "arquivos_para_renomear_gpx = sorted([arquivo for arquivo in os.listdir(DOWNLOADS) if arquivo.endswith(\".gpx\")], reverse=True)\n",
    "arquivos_para_renomear_gpx\n",
    "\n",
    "\n",
    "#for user, arquivo_para_renomear_gpx in zip(users, arquivos_para_renomear_gpx):\n",
    "users = ['CHEVROLET_BOLT_EUV_2022','CHEVROLET_MALIBU_2022','CHEVROLET_EQUINOX_2022','CHEVROLET_SILVERADO_1500','CHRYSLER_PACIFICA_HYBRID_2019','FORD_MUSTANG','FORD_F-150','FORD_EXPLORER','HONDA_ACCORD_2018','HONDA_ACCORD_HYBRID_2018','HONDA_CIVIC_2016','HONDA_CIVIC_2022','HONDA_PILOT_2017','HYUNDAI_IONIQ_5_2022','HYUNDAI_IONIQ_PHEV_2020','HYUNDAI_KONA_ELECTRIC_2019','HYUNDAI_PALISADE_2020','HYUNDAI_SANTA_FE_2019','HYUNDAI_SANTA_FE_HYBRID_2022','HYUNDAI_SONATA_2020','KIA_EV6_2022','KIA_NIRO_EV_2020','LEXUS_ES_2016','LEXUS_RX_2016','LEXUS_RX_2020','RAM_1500_5TH_GEN','TOYOTA_CAMRY_HYBRID_2021','TOYOTA_COROLLA_HYBRID_TSS2_2019','TOYOTA_COROLLA_TSS2_2019','TOYOTA_HIGHLANDER_2020','TOYOTA_PRIUS_2017','TOYOTA_PRIUS_2017-20','TOYOTA_PRIUS_v_2017','TOYOTA_RAV4_2019','TOYOTA_RAV4_HYBRID_2019','VOLKSWAGEN_GOLF_7TH_GEN','VOLKSWAGEN_PASSAT_2022','VOLKSWAGEN_TIGUAN_2022','VOLKSWAGEN_ATLAS_CROSS_SPORT']\n",
    "prefix = '__routes'\n",
    "for arquivo_para_renomear_gpx in arquivos_para_renomear_gpx:\n",
    "        if os.listdir(DOWNLOADS):\n",
    "                user = random.choice(users)\n",
    "                caminho_antigo = os.path.join(DOWNLOADS, arquivo_para_renomear_gpx)        \n",
    "                #novo_nome = f\"{arquivo_para_renomear_gpx.replace('.gpx', '')}__{user}.gpx\"\n",
    "                novo_nome = f\"{arquivo_para_renomear_gpx.replace('.gpx', '')}{prefix}__{user}.gpx\"                \n",
    "                caminho_novo = os.path.join(DOWNLOADS,novo_nome)        \n",
    "                #print(novo_nome)\n",
    "                os.rename(caminho_antigo, caminho_novo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movendo os arquivos para a pasta PRE_PROCESSING\n",
    "#time.sleep(3)\n",
    "arquivos_para_pre_processamento = [arquivo for arquivo in os.listdir(DOWNLOADS) if arquivo.endswith(\".gpx\")] # Listando arquivos com extensão [.gpx] (agora renomeados) na pasta de download.\n",
    "for arquivo_para_pre_processamento in arquivos_para_pre_processamento: # Iterando sobre a lista de arquivos na pasta download que serão movidos para a pasta de pré-processamento.\n",
    "    caminho_origem = os.path.join(DOWNLOADS, arquivo_para_pre_processamento) # Montando o caminho absoluto da pasta de origem dos arquivos (download + arquivo)\n",
    "    caminho_destino = os.path.join(PRE_PROCESSAMENTO, arquivo_para_pre_processamento) # Montando o caminho absoluto da pasta de destino dos arquivos (PRE_PROCESSING + arquivo)\n",
    "    \n",
    "    try: # Adicionado tratamento de erro para caso não seja possivel a transferencia\n",
    "        os.rename(caminho_origem,caminho_destino) # Efetivando a transferencia do arquivo da pasta de download para a pasta PRE_PROCESSING\n",
    "    except Exception as e: #--> Capturando qualquer erro que porventura ocorra\n",
    "        print(f\"Erro ao mover o arquivo: '{arquivo_para_pre_processamento}': {e}.\") #--> Exibindo o erro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ª Função. Responsável por fazer o download das rotas diretamente site, renomear os arquivos e os mover para a pasta de pré processamento - Essa execução deve ser executada de uma única vez, pois pode haver alguma falha no momento de renomear os arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "# pip install webdriver-manager\n",
    "# pip install BeautifulSoup4\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "servico = Service(ChromeDriverManager().install())\n",
    "import re\n",
    "import os\n",
    "\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# import requests\n",
    "# options = Options()\n",
    "# options.add_argument('--headless') #--> Parâmetro adicional onde é possivel realizar o webscrapping sem abrir o navegador.\n",
    "# options.add_argument('window-size=400,800') #--> Parâmetro adicional onde é possivel escolher o tamanho da tela aberta no navegador. Nesse exemplo o tamanho 400x800 é como se estivesse aberto em um celular.\n",
    "# navegador = webdriver.Chrome(service=servico, options=options) #--> Aplicando as opções acima mencionadas no navegador.\n",
    "\n",
    "URL_OPEN_STREET_MAP_TRACES = 'https://www.openstreetmap.org/traces' #--> Página do OpenstreetMap onde estão localizadas as rotas para download.\n",
    "PREFIXO_URL_DOWNLOAD = 'https://www.openstreetmap.org' #--> Página do principal do OpenstreetMap. Esta variável será utilizada para montar a URL das páginas de download.\n",
    "DOWNLOADS = '/home/thiago/Downloads/'\n",
    "PRE_PROCESSAMENTO = '/home/thiago/tcc_ufrj/PRE_PROCESSAMENTO'\n",
    "\n",
    "navegador = webdriver.Chrome(service=servico)\n",
    "navegador.get(URL_OPEN_STREET_MAP_TRACES)\n",
    "\n",
    "conteudo_da_pagina = navegador.page_source\n",
    "site = BeautifulSoup(conteudo_da_pagina, 'html.parser')\n",
    "rotas = site.findAll('tr')\n",
    "\n",
    "lista_rotas_pendentes = []\n",
    "lista_rotas_finalizadas = []\n",
    "lista_rotas = []\n",
    "\n",
    "for rota in rotas:\n",
    "    if rota.find('span', attrs={'class': 'text-danger'}):\n",
    "        rotas_pendentes = rota.find('span', attrs={'class': 'text-danger'})    \n",
    "        link_rotas_pendentes = rota.find('a')        \n",
    "        lista_rotas.append([PREFIXO_URL_DOWNLOAD+link_rotas_pendentes['href']])\n",
    "        # lista_rotas_pendentes.append([PREFIX_URL_DOWNLOAD+link_rotas_pendentes['href']])\n",
    "        print (f\"{rotas_pendentes.text}: {PREFIXO_URL_DOWNLOAD+link_rotas_pendentes['href']}\")\n",
    "\n",
    "    else:         \n",
    "        link_rotas_finalizadas = rota.find('a')        \n",
    "        lista_rotas.append([PREFIXO_URL_DOWNLOAD+link_rotas_finalizadas['href']])\n",
    "        # lista_rotas_finalizadas.append([PREFIX_URL_DOWNLOAD+link_rotas_finalizadas['href']])\n",
    "        print(f\"FINISHED: {PREFIXO_URL_DOWNLOAD+link_rotas_finalizadas['href']}\")\n",
    "time.sleep(3)\n",
    "navegador.close()\n",
    "\n",
    "#len(list_rotas_pendentes) #--> Verificando a quantidade de registros na lista de rotas pendentes\n",
    "#len(list_rotas_finalizadas) #--> Verificando a quantidade de registros na lista de rotas pendentes\n",
    "\n",
    "## Quando usa [requests.get] estamos usando o BeautifulSoup - e quando usamos só o [.get] estamos usando o selenium\n",
    "#navegador = webdriver.Chrome(service=servico)\n",
    "\n",
    "\n",
    "## Baixando os arquivos \n",
    "navegador = webdriver.Chrome(service=servico)\n",
    "usuarios = []\n",
    "for lista_rota in lista_rotas:\n",
    "    time.sleep(3)\n",
    "    url = lista_rota[0]\n",
    "    navegador.get(url) #--> Exemplo onde usamos o Selenium somente com o [.get]\n",
    "\n",
    "    conteudo_pagina_download = navegador.page_source\n",
    "    pagina_usuario = BeautifulSoup(conteudo_pagina_download, 'html.parser')\n",
    "\n",
    "    if any(td.find('span', attrs={'class': 'text-danger'}) for td in pagina_usuario):\n",
    "        tb_nome_usuario = navegador.find_element('xpath', '//*[@id=\"content\"]/div[2]/div/table/tbody/tr[4]/td')\n",
    "        nome_usuario = tb_nome_usuario.text\n",
    "        nome_usuario = re.sub(r'\\s|\\.|\\(|\\)','_',nome_usuario)        \n",
    "        print(f'Rota PENDENTE: {lista_rota} Usuário: {nome_usuario}')\n",
    "        navegador.find_element('xpath','//*[@id=\"content\"]/div[2]/div/table/tbody/tr[1]/td/a').click()\n",
    "        usuarios.append(nome_usuario)\n",
    "\n",
    "    else:\n",
    "        tb_nome_usuario = navegador.find_element('xpath', '//*[@id=\"content\"]/div[2]/div/table/tbody/tr[6]/td')\n",
    "        nome_usuario = tb_nome_usuario.text\n",
    "        nome_usuario = re.sub(r'\\s|\\.|\\(|\\)','_',nome_usuario)\n",
    "        print(f'Rota FINALIZADA: {lista_rota} Usuário: {nome_usuario}')\n",
    "        navegador.find_element('xpath','//*[@id=\"content\"]/div[2]/div/table/tbody/tr[1]/td/a').click()\n",
    "        usuarios.append(nome_usuario)\n",
    "time.sleep(5)\n",
    "navegador.close()\n",
    "\n",
    "\n",
    "## Corrigindo o nome dos arquivos conforme seus usuários\n",
    "arquivos_para_renomear = [arquivo for arquivo in os.listdir(DOWNLOADS) if arquivo.endswith(\".crdownload\")]\n",
    "for arquivo_para_renomear in arquivos_para_renomear:\n",
    "        novo_nome = arquivo_para_renomear.replace(\".crdownload\", \"\")\n",
    "        os.rename(os.path.join(DOWNLOADS, arquivo_para_renomear), os.path.join(DOWNLOADS, novo_nome))\n",
    "\n",
    "arquivos_para_renomear_gpx = sorted([arquivo for arquivo in os.listdir(DOWNLOADS) if arquivo.endswith(\".gpx\")], reverse=True)\n",
    "arquivos_para_renomear_gpx\n",
    "\n",
    "users = ['CHEVROLET_BOLT_EUV_2022','CHEVROLET_MALIBU_2022','CHEVROLET_EQUINOX_2022','CHEVROLET_SILVERADO_1500','CHRYSLER_PACIFICA_HYBRID_2019','FORD_MUSTANG','FORD_F-150','FORD_EXPLORER','HONDA_ACCORD_2018','HONDA_ACCORD_HYBRID_2018','HONDA_CIVIC_2016','HONDA_CIVIC_2022','HONDA_PILOT_2017','HYUNDAI_IONIQ_5_2022','HYUNDAI_IONIQ_PHEV_2020','HYUNDAI_KONA_ELECTRIC_2019','HYUNDAI_PALISADE_2020','HYUNDAI_SANTA_FE_2019','HYUNDAI_SANTA_FE_HYBRID_2022','HYUNDAI_SONATA_2020','KIA_EV6_2022','KIA_NIRO_EV_2020','LEXUS_ES_2016','LEXUS_RX_2016','LEXUS_RX_2020','RAM_1500_5TH_GEN','TOYOTA_CAMRY_HYBRID_2021','TOYOTA_COROLLA_HYBRID_TSS2_2019','TOYOTA_COROLLA_TSS2_2019','TOYOTA_HIGHLANDER_2020','TOYOTA_PRIUS_2017','TOYOTA_PRIUS_2017-20','TOYOTA_PRIUS_v_2017','TOYOTA_RAV4_2019','TOYOTA_RAV4_HYBRID_2019','VOLKSWAGEN_GOLF_7TH_GEN','VOLKSWAGEN_PASSAT_2022','VOLKSWAGEN_TIGUAN_2022','VOLKSWAGEN_ATLAS_CROSS_SPORT']\n",
    "prefix = '__routes'\n",
    "\n",
    "# for usuario, arquivo_para_renomear_gpx in zip(usuarios, arquivos_para_renomear_gpx):        \n",
    "#     caminho_antigo = os.path.join(DOWNLOADS, arquivo_para_renomear_gpx)\n",
    "#     novo_nome = f\"{arquivo_para_renomear_gpx.replace('.gpx', '')}__{usuario}.gpx\"        \n",
    "#     caminho_novo = os.path.join(DOWNLOADS,novo_nome)            \n",
    "#     os.rename(caminho_antigo, caminho_novo)\n",
    "\n",
    "for arquivo_para_renomear_gpx in arquivos_para_renomear_gpx:    \n",
    "    user = random.choice(users)\n",
    "    caminho_antigo = os.path.join(DOWNLOADS, arquivo_para_renomear_gpx)\n",
    "    novo_nome = f\"{arquivo_para_renomear_gpx.replace('.gpx', '')}{prefix}__{user}.gpx\"                \n",
    "    caminho_novo = os.path.join(DOWNLOADS,novo_nome) # Montando o novo caminho absoluto da pasta download + arquivo renomeado\n",
    "    os.rename(caminho_antigo, caminho_novo) # Efetivando a renomeação do arquivo antigo pelo novo\n",
    "\n",
    "\n",
    "# Movendo os arquivos para a pasta PRE_PROCESSING\n",
    "time.sleep(3)\n",
    "arquivos_para_pre_processamento = [arquivo for arquivo in os.listdir(DOWNLOADS) if arquivo.endswith(\".gpx\")]\n",
    "for arquivo_para_pre_processamento in arquivos_para_pre_processamento:\n",
    "    caminho_origem = os.path.join(DOWNLOADS, arquivo_para_pre_processamento)\n",
    "    caminho_destino = os.path.join(PRE_PROCESSAMENTO, arquivo_para_pre_processamento)\n",
    "    try:\n",
    "        os.rename(caminho_origem,caminho_destino)\n",
    "        print(f\"Arquivo '{arquivo_para_pre_processamento}' movido para a pasta {PRE_PROCESSAMENTO}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao mover o arquivo: '{arquivo_para_pre_processamento}': {e}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours, remainder = divmod(execution_time, 3600)\n",
    "minutes, remainder = divmod(remainder, 60)\n",
    "seconds, milliseconds = divmod(remainder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tempo de execução: {int(hours)} horas, {int(minutes)} minutos, {int(seconds)} segundos e {int(milliseconds * 1000)} milissegundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2ª Função - Lendo arquivos pasta de Pré Processamento - Convertendo os arquivos .gpx em .csv - Levando os arquivos .csv para a camada bronze do datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "### IMPORTAÇÃO DAS BIBLIOTECAS NECESSÁRIAS ###\n",
    "##############################################\n",
    "# pip install minio \n",
    "# pip install gpxpy\n",
    "# pip install pandas \n",
    "\n",
    "import gpxpy \n",
    "import gpxpy.gpx \n",
    "import pandas as pd \n",
    "import os \n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "##############################\n",
    "### DEFINIÇÃO DE VARIÁVEIS ### \n",
    "##############################\n",
    "PRE_PROCESSAMENTO = '/home/thiago/tcc_ufrj/PRE_PROCESSAMENTO' \n",
    "CAMADA_BRONZE = 'bronze' \n",
    "\n",
    "##############################################\n",
    "### CRIANDO UMA INSTÂNCIA DO CLIENTE MINIO ###\n",
    "##############################################\n",
    "minioclient = Minio('localhost:9000', \n",
    "    access_key='minioadmin', \n",
    "    secret_key='minioadmin', \n",
    "    secure=False) \n",
    "\n",
    "###########################################################\n",
    "### CONVERTENDO OS ARQUIVOS COM A EXTENSÃO .GPX EM .CSV ###\n",
    "###########################################################\n",
    "arquivos_pre_processamento = [arquivo for arquivo in os.listdir(PRE_PROCESSAMENTO) if arquivo.endswith(\".gpx\")] \n",
    "for arquivo_pre_processamento in arquivos_pre_processamento: \n",
    "    caminho_arquivo = os.path.join(PRE_PROCESSAMENTO, arquivo_pre_processamento) \n",
    "    try:\n",
    "        with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo_gpx: \n",
    "            gpx = gpxpy.parse(arquivo_gpx)\n",
    "\n",
    "        info_rota = [] \n",
    "        for trilha in gpx.tracks: \n",
    "            for segmento in trilha.segments:\n",
    "                for ponto in segmento.points:\n",
    "                    info_rota.append({\n",
    "                        'latitude': ponto.latitude, \n",
    "                        'longitude': ponto.longitude, \n",
    "                        'elevacao' : ponto.elevation, \n",
    "                        'time_point' : ponto.time \n",
    "                    })\n",
    "        arquivo_pre_processamento_csv = arquivo_pre_processamento.replace('.gpx','.csv') \n",
    "        info_rota_df = pd.DataFrame(info_rota) \n",
    "        info_rota_df.to_csv(f'{PRE_PROCESSAMENTO}/{arquivo_pre_processamento_csv}', index=False) \n",
    "        os.remove(caminho_arquivo) \n",
    "    except Exception as e: \n",
    "        print(f\"Erro ao converter o '{arquivo_pre_processamento}': {e}.\") \n",
    "        os.remove(caminho_arquivo)\n",
    "        print(f\"Excluindo o arquivo defeituoso: {arquivo_pre_processamento}\")\n",
    "        \n",
    "\n",
    "################################################################################################################\n",
    "### MOVENDO OS ARQUIVOS COM A EXTENSÃO .CSV DA PASTA PRE_PROCESSAMENTO PARA A PRIMEIRA CAMADA (BRONZE) NO MINIO ###\n",
    "################################################################################################################\n",
    "arquivos_para_datalake = [arquivo for arquivo in os.listdir(PRE_PROCESSAMENTO) if arquivo.endswith(\".csv\")] \n",
    "for nome_arquivo in arquivos_para_datalake: \n",
    "    caminho_pre_proc = os.path.join(PRE_PROCESSAMENTO, nome_arquivo) \n",
    "    if os.path.isfile(caminho_pre_proc): \n",
    "        try:\n",
    "            minioclient.fput_object(CAMADA_BRONZE, nome_arquivo, caminho_pre_proc) \n",
    "            print(f\"Arquivo {nome_arquivo} enviado com sucesso para o bucket.\") \n",
    "            os.remove(caminho_pre_proc) \n",
    "        except S3Error as e: \n",
    "            print(f\"Erro ao enviar o arquivo: {nome_arquivo} -> Erro: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/thiago/tcc_ufrj/PRE_PROCESSAMENTO/10086735__Routes_from_dragonpilot_2023_08_15__TOYOTA_RAV4_HYBRID_2019__.gpx', 'r', encoding='utf-8') as arquivo_gpx:\n",
    "    gpx = gpxpy.parse(arquivo_gpx)\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "info_rota = [] #--> Lista que servirá de apoio para converter o arquivo .gpx em uma lista\n",
    "for trilha in gpx.tracks:\n",
    "    for segmento in trilha.segments:\n",
    "        for ponto in segmento.points:            \n",
    "            \n",
    "            latitude = ponto.latitude\n",
    "            longitude = ponto.longitude\n",
    "\n",
    "            address = location.raw['address']\n",
    "            city = address.get('city', '')\n",
    "            state = address.get('state', '')\n",
    "            country = address.get('country', '')\n",
    "\n",
    "            location = geolocator.reverse(f'{latitude},{longitude}')\n",
    "            address = location.raw['address']\n",
    "            cidade = address.get('suburb')\n",
    "            estado = address.get('state')\n",
    "            pais = address.get('country_code')\n",
    "\n",
    "            info_rota.append({\n",
    "                'latitude': ponto.latitude,\n",
    "                'longitude': ponto.longitude,\n",
    "                'elevacao' : ponto.elevation,\n",
    "                'time_point' : ponto.time,\n",
    "                'cidade': cidade,\n",
    "                'estado': estado,\n",
    "                'pais': pais\n",
    "            })\n",
    "arquivo_pre_processamento_csv = arquivo_pre_processamento.replace('.gpx','.csv')\n",
    "info_rota_df = pd.DataFrame(info_rota)\n",
    "## os.remove(caminho_arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "location = geolocator.reverse(f'{Latitude},{Longitude}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "Latitude = \"45.24559097710776\"\n",
    "Longitude = \"-122.79781984274484\"\n",
    "location = geolocator.reverse(f'{Latitude},{Longitude}') \n",
    "\n",
    "\n",
    "address = location.raw['address']\n",
    "cidade = address.get('suburb')\n",
    "estado = address.get('ISO3166-2-lvl4')\n",
    "pais = address.get('country_code')\n",
    "\n",
    "estado = estado[-2:]\n",
    "pais = pais.upper()\n",
    "\n",
    "print(f\"Cidade: {cidade}\")\n",
    "print(f\"Estado (abreviado): {estado[-2:]}\")\n",
    "print(f\"País: {pais}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('arquivo.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADRAO_REGEX = r'(.*?)__'\n",
    "padrao_encontrado = re.findall(PADRAO_REGEX, nome_arquivo)\n",
    "id_rota = padrao_encontrado[0]\n",
    "nome_usuario = padrao_encontrado[-1]\n",
    "padrao_encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_para_datalake = [files for files in os.listdir(PRE_PROCESSAMENTO) if files.endswith(\".csv\")]\n",
    "#arquivos_para_datalake = [files for files in os.listdir(PRE_PROCESSAMENTO) if files.endswith(\".parquet\")]\n",
    "len(arquivos_para_datalake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aqui os arquivos com a extensão .csv saem da pasta PRE_PROCESSING para a primeira camada (bronze) no MinIO\n",
    "#arquivos_para_datalake = [files for files in os.listdir(PRE_PROCESSING) if files.endswith(\".csv\")]\n",
    "arquivos_para_datalake = [files for files in os.listdir(PRE_PROCESSING) if files.endswith(\".parquet\")]\n",
    "for nome_arquivo in arquivos_para_datalake:\n",
    "    caminho_pre_proc = os.path.join(PRE_PROCESSING, nome_arquivo)    \n",
    "    if os.path.isfile(caminho_pre_proc):\n",
    "        try:\n",
    "            minioclient.fput_object(BRONZE_LAYER, nome_arquivo, caminho_pre_proc)\n",
    "            print(f\"Arquivo {nome_arquivo} enviado com sucesso para o bucket.\")\n",
    "            os.remove(caminho_pre_proc) # --> Após o envio bem sucedido para o bucket o arquivo é excluído da pasta download\n",
    "        except S3Error as e:\n",
    "            print(f\"Erro ao enviar o arquivo: {nome_arquivo} -> Erro: {e}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PRE_PROCESSING/9632214__Routes_from_sunnypilot_2023_08_19-dev__SUBARU_IMPREZA_LIMITED_2019__.gpx', 'r', encoding='utf-8') as arquivo_gpx:\n",
    "    gpx = gpxpy.parse(arquivo_gpx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_gpx = '9632229__Routes_from_sunnypilot_2023_08_19-dev__HYUNDAI_SANTA_FE_2019__.gpx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_gpx = str(arquivo_gpx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arquivo_gpx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(arquivo_gpx, 'r', encoding='utf-8') as arquivo_gpx:\n",
    "    gpx = gpxpy.parse(arquivo_gpx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gpx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para retornar a quantidade de pontos (lat+lon+ele) em um arquivo arquivo '.gpx' podemos usar get\n",
    "# Documentação: https://pypi.org/project/gpxpy/\n",
    "gpx.get_track_points_no()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para retornar a faixa de altitude, a fim de se obter extremos de maior elevação e menor elevação no trajeto percorrido - os valores apresentados são em metros acima do nível do mar\n",
    "gpx.get_elevation_extremes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não endendi para que isso serve\n",
    "gpx.get_uphill_downhill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo o nome do criador da rota - Entretanto no nosso caso quando usamos esse comando temos uma informação generica - Foi necessário obter o nome do criador da rota com o WebScraping\n",
    "creator = gpx.creator\n",
    "creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para exibir o conteudo do arquivo .gpx em formato xml\n",
    "print(gpx.to_xml()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# É possivel verificar quantas rotas/trilhas nosso arquivo .gpx possui\n",
    "len(gpx.tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como no nosso exemplo acima só temos 1 rota/trilha efetuada, podemos acessar por meio de indice no python, \n",
    "# Nesse caso se passa o valor [0] para ter uma precisão exata da rota em que estamos trabalhando\n",
    "gpx.tracks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vamos acessar os segmentos da nossa rota/trilha\n",
    "gpx.tracks[0].segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora podemos acessar os pontos de dados individuais dentro do nosso gpx acessando a matriz de pontos. Aqui tambem podemo0s ver o nome das propriedades do arquivo como elevation e time\n",
    "gpx.tracks[0].segments[0].points[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui acessamos as tres camadas: Track - Segments e Point e o passamos para um array em forma de dicionário. Note que podemos acessar cada atributo. No meu caso estou pegando latitude, Longitude, Elevação e a hora de cada ponto (entenda-se como ponto Lat+Long+Ele)\n",
    "info_rota = []\n",
    "for track in gpx.tracks:\n",
    "    for segment in track.segments:\n",
    "        for point in segment.points:\n",
    "            info_rota.append({\n",
    "                'latitude': point.latitude,\n",
    "                'longitude': point.longitude,\n",
    "                'elevacao' : point.elevation,\n",
    "                'time_point' : point.time\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo somente os 3 primeiros resultados\n",
    "info_rota[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(info_rota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui podemos transformar essa informação em um dataframe com a biblioteca pandas\n",
    "info_rota_df = pd.DataFrame(info_rota)\n",
    "info_rota_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(info_rota_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui salvamos o pandas dataframe localmente\n",
    "info_rota_df.to_csv('pandas_info_rota.csv', index=False)\n",
    "\n",
    "# <creator> contem o nome do cara que está se deslocando\n",
    "# existe uma <trkpt> acredito q seja um track point\n",
    "# <name> contem uma da e hora. Acredito que seja a hora do envio do arquivo gpx\n",
    "# <trkpt> é um bloco com lat - lon - internamente tem um <ele> de elevação e um <time> que acredito ser o horario que o cliente estava no ponto lat+long+ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui salvamos o pandas dataframe localmente\n",
    "info_rota_df.to_parquet('pd_parquet_info_rota.parquet', index=False)\n",
    "\n",
    "# <creator> contem o nome do cara que está se deslocando\n",
    "# existe uma <trkpt> acredito q seja um track point\n",
    "# <name> contem uma da e hora. Acredito que seja a hora do envio do arquivo gpx\n",
    "# <trkpt> é um bloco com lat - lon - internamente tem um <ele> de elevação e um <time> que acredito ser o horario que o cliente estava no ponto lat+long+ele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3ª Função - Lendo o bucket e transformando os arquivos encontrados lá em DF Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install minio\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, cityblock, minkowski\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CAMADA_BRONZE = 'bronze'\n",
    "CAMADA_SILVER = 'silver'\n",
    "PADRAO_REGEX = r'(.*?)__'\n",
    "\n",
    "PADRAO_REGEX_1 = r'(\\d+)__(.*?)\\.csv'\n",
    "PADRAO_REGEX_2 = r'(.*?)__'\n",
    "PADRAO_REGEX_3 = '__.+'\n",
    "\n",
    "\n",
    "\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_BRONZE) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "arquivos_rotas_gpx_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:\n",
    "    try:\n",
    "\n",
    "        ### OBTENDO O ARQUIVO E O CONVERTENDO EM UM DF PANDAS ###\n",
    "        obj_rota_csv = minioclient.get_object(CAMADA_BRONZE, arquivo_rotas_gpx_csv.object_name)        \n",
    "        csv_decod = obj_rota_csv.data.decode('utf-8')  # Convertendo bytes para string    \n",
    "        arquivo_csv = StringIO(csv_decod)\n",
    "        df = pd.read_csv(arquivo_csv, sep=';')\n",
    "\n",
    "        #### SEPARANDO A INFORMAÇÃO DE DATA E HORA EM 2 COLUNAS SEPARADAS ###\n",
    "        df['time_point'] = pd.to_datetime(df['time_point'], format='%Y-%m-%d %H:%M:%S', errors='coerce')        \n",
    "\n",
    "        df['data'] = df['time_point'].dt.date\n",
    "        df['hora'] = df['time_point'].dt.strftime('%H:%M:%S')\n",
    "               \n",
    "        \n",
    "        df = df.drop(columns=['time_point'])\n",
    "    except S3Error as e:\n",
    "        print(f\"Erro ao alterar a data do arquivo: {nome_arquivo}. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:\n",
    "\n",
    "\n",
    "    ### OBTENDO O ARQUIVO E O CONVERTENDO EM UM DF PANDAS ###\n",
    "    obj_rota_csv = minioclient.get_object(CAMADA_BRONZE, arquivo_rotas_gpx_csv.object_name)        \n",
    "    csv_decod = obj_rota_csv.data.decode('utf-8')  # Convertendo bytes para string    \n",
    "    arquivo_csv = StringIO(csv_decod)\n",
    "    df = pd.read_csv(arquivo_csv, sep=';')\n",
    "\n",
    "    #### SEPARANDO A INFORMAÇÃO DE DATA E HORA EM 2 COLUNAS SEPARADAS ###\n",
    "    df['time_point'] = pd.to_datetime(df['time_point'], errors='coerce')\n",
    "    df['data'] = df['time_point'].dt.date\n",
    "    df['hora'] = df['time_point'].dt.strftime('%H:%M:%S')\n",
    "    df = df.drop(columns=['time_point'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df['time_point'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_BRONZE) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "arquivos_rotas_gpx_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_BRONZE) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:\n",
    "\n",
    "    ### OBTENDO O ARQUIVO E O CONVERTENDO EM UM DF PANDAS ###\n",
    "    obj_rota_csv = minioclient.get_object(CAMADA_BRONZE, arquivo_rotas_gpx_csv.object_name)        \n",
    "    csv_decod = obj_rota_csv.data.decode('utf-8')  # Convertendo bytes para string    \n",
    "    arquivo_csv = StringIO(csv_decod)\n",
    "    df = pd.read_csv(arquivo_csv, sep=';')\n",
    "\n",
    "    ### SEPARANDO A INFORMAÇÃO DE DATA E HORA EM 2 COLUNAS SEPARADAS ###\n",
    "    # df['time_point'] = pd.to_datetime(df['time_point'])\n",
    "    # df['data'] = df['time_point'].dt.date\n",
    "    # df['hora'] = df['time_point'].dt.strftime('%H:%M:%S')\n",
    "    # df = df.drop(columns=['time_point']) \n",
    "\n",
    "    ### USANDO O REGEX PARA PEGAR O ID DA RODA E O NOME DO USUÁRIO (DADOS PRESENTES NO NOME DO ARQUIVO) ###\n",
    "    nome_arquivo = arquivo_rotas_gpx_csv.object_name        \n",
    "    padrao_encontrado = re.findall(PADRAO_REGEX_3, nome_arquivo)\n",
    "    id_rota = padrao_encontrado[0][0]\n",
    "    nome_usuario = padrao_encontrado[0][1]\n",
    "    num_df = len(df)\n",
    "    replic_id_rota = np.tile(id_rota, num_df)\n",
    "    replic_nome_usuario = np.tile(nome_usuario, num_df)\n",
    "    # df['id_rota'] = replic_id_rota\n",
    "    # df['nome_usuario'] = replic_nome_usuario\n",
    "    # ordenacao_df = ['id_rota', 'nome_usuario', 'latitude', 'longitude', 'elevacao', 'data', 'hora']\n",
    "    # df = df.reindex(columns=ordenacao_df)\n",
    "\n",
    "    #try:\n",
    "    #    csv_bytes = df.to_csv(index=False).encode('utf-8') \n",
    "    #    csv_buffer = BytesIO(csv_bytes)\n",
    "    #    minioclient.put_object(\n",
    "    #                        CAMADA_SILVER,\n",
    "    #                        nome_arquivo,\n",
    "    #                        data=csv_buffer,\n",
    "    #                        length=len(csv_bytes),\n",
    "    #                        content_type='application/csv')\n",
    "\n",
    "        #minioclient.remove_object(CAMADA_BRONZE, nome_arquivo)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_rota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADRAO_REGEX_1 = r'(\\d+)__(.*?)\\.csv'\n",
    "PADRAO_REGEX_2 = r'(.*?)__'\n",
    "PADRAO_REGEX_3 = '__.+'\n",
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_BRONZE) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:\n",
    "    nome_arquivo = arquivo_rotas_gpx_csv.object_name\n",
    "    padrao_encontrado = re.search(PADRAO_REGEX_1, nome_arquivo)  \n",
    "    if padrao_encontrado:\n",
    "        id_rota = padrao_encontrado.group(1)        \n",
    "        nome_usuario = padrao_encontrado.group(2)        \n",
    "        substrings = re.findall(PADRAO_REGEX_3, nome_usuario)                \n",
    "        nome_usuario = [substring.replace('__','') for substring in substrings]\n",
    "        print(nome_usuario)\n",
    "\n",
    "#        if substrings:\n",
    "#            nome_usuario = substrings[-1]\n",
    "#        else:\n",
    "#            nome_usuario = nome_usuario       \n",
    "#        print(f\"ID da Rota: {id_rota}\")\n",
    "#        print(f\"Nome do Usuário: {nome_usuario}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padrao_encontrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_rota_csv = minioclient.get_object(BRONZE_LAYER, '9781413__Routes_from_dragonpilot_2023_07_05__TOYOTA_COROLLA_TSS2_2019__.csv')\n",
    "rota_csv = blob_rota_csv.data\n",
    "rota_csv = rota_csv.decode(\"ISO-8859-1\")\n",
    "pre_df = StringIO(rota_csv)\n",
    "df = pd.read_csv(pre_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_point'] = pd.to_datetime(df['time_point'])\n",
    "df['data'] = df['time_point'].dt.date\n",
    "df['hora'] = df['time_point'].dt.strftime('%H:%M:%S')\n",
    "df = df.drop(columns=['time_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "text = \"9781892__Routes_from_sunnypilot_0_9_3_1-release__HONDA_ACCORD_2018__.gpx\"\n",
    "\n",
    "# Definindo o padrão de regex\n",
    "pattern = r'(.*?)__'\n",
    "\n",
    "# Encontrando todas as correspondências no texto\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "# Extraindo o nome do carro\n",
    "username = matches[-1]\n",
    "id = matches[0]\n",
    "\n",
    "print(f'Id: {id} - username: {username}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = len(df)\n",
    "num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_id = np.tile(id, num_df)\n",
    "repeated_user = np.tile(username, num_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = repeated_id\n",
    "df['username'] = repeated_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordenando as colunas\n",
    "desired_order = ['id', 'username', 'latitude', 'longitude', 'elevacao', 'data', 'hora']\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('arquivo.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Suas strings de exemplo\n",
    "strings = [\n",
    "    \"10109849__Muara_Bulian.csv\",\n",
    "    \"10109852__Routes_from_sunnypilot_0_9_4_1-release__VOLKSWAGEN_PASSAT_8TH_GEN__.csv\",\n",
    "    \"10127398__Jambi_Paradise_-_Simpang_Paal_Merah.csv\"\n",
    "]\n",
    "\n",
    "# Padrão regex\n",
    "PADRAO_REGEX_1 = r'(\\d+)__(.*?)\\.csv'\n",
    "PADRAO_REGEX_2 = r'(.*?)__'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nome_arquivo in strings:\n",
    "    padrao_encontrado = re.search(PADRAO_REGEX_1, nome_arquivo)\n",
    "    if padrao_encontrado:\n",
    "        id_rota = padrao_encontrado.group(1)\n",
    "        nome_usuario = padrao_encontrado.group(2)\n",
    "\n",
    "        substrings = re.findall(PADRAO_REGEX_2, nome_usuario)\n",
    "        if substrings:\n",
    "            nome_usuario = substrings[-1]\n",
    "        else:\n",
    "            nome_usuario = nome_usuario\n",
    "       \n",
    "        print(f\"ID da Rota: {id_rota}\")\n",
    "        print(f\"Nome do Usuário: {nome_usuario}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Suas strings de exemplo\n",
    "strings = [\n",
    "    \"10109849__Muara_Bulian.csv\",\n",
    "    \"10109852__Routes_from_sunnypilot_0_9_4_1-release__VOLKSWAGEN_PASSAT_8TH_GEN__.csv\",\n",
    "    \"10127398__Jambi_Paradise_-_Simpang_Paal_Merah.csv\"\n",
    "]\n",
    "\n",
    "# Padrão regex\n",
    "PADRAO_REGEX_1 = r'(\\d+)__(.*?)\\.csv'\n",
    "PADRAO_REGEX_2 = r'(.*?)__'\n",
    "\n",
    "for nome_arquivo in strings:\n",
    "    padrao_encontrado = re.search(PADRAO_REGEX_1, nome_arquivo)\n",
    "    if padrao_encontrado:\n",
    "        id_rota = padrao_encontrado.group(1)\n",
    "        nome_usuario = padrao_encontrado.group(2)\n",
    "\n",
    "        substrings = re.findall(PADRAO_REGEX_2, nome_usuario)\n",
    "        if substrings:\n",
    "            nome_usuario = substrings[-1]\n",
    "        else:\n",
    "            nome_usuario = nome_usuario\n",
    "       \n",
    "        print(f\"ID da Rota: {id_rota}\")\n",
    "        print(f\"Nome do Usuário: {nome_usuario}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_rota</th>\n",
       "      <th>nome_usuario</th>\n",
       "      <th>lat2</th>\n",
       "      <th>lon2</th>\n",
       "      <th>elevacao</th>\n",
       "      <th>data</th>\n",
       "      <th>hora</th>\n",
       "      <th>cidade</th>\n",
       "      <th>estado</th>\n",
       "      <th>pais</th>\n",
       "      <th>data_carga_banco</th>\n",
       "      <th>lat1</th>\n",
       "      <th>lon1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405903</td>\n",
       "      <td>-100.134304</td>\n",
       "      <td>456.357153</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405721</td>\n",
       "      <td>-100.134209</td>\n",
       "      <td>456.267968</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405903</td>\n",
       "      <td>-100.134304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405539</td>\n",
       "      <td>-100.134116</td>\n",
       "      <td>456.450866</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405721</td>\n",
       "      <td>-100.134209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405356</td>\n",
       "      <td>-100.134023</td>\n",
       "      <td>456.304257</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405539</td>\n",
       "      <td>-100.134116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405174</td>\n",
       "      <td>-100.133930</td>\n",
       "      <td>456.173852</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405356</td>\n",
       "      <td>-100.134023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658215</td>\n",
       "      <td>-100.372543</td>\n",
       "      <td>603.861116</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658173</td>\n",
       "      <td>-100.372397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658258</td>\n",
       "      <td>-100.372691</td>\n",
       "      <td>603.889631</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658215</td>\n",
       "      <td>-100.372543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658302</td>\n",
       "      <td>-100.372841</td>\n",
       "      <td>604.039239</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658258</td>\n",
       "      <td>-100.372691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658348</td>\n",
       "      <td>-100.372990</td>\n",
       "      <td>604.363518</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658302</td>\n",
       "      <td>-100.372841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658391</td>\n",
       "      <td>-100.373137</td>\n",
       "      <td>604.472864</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658348</td>\n",
       "      <td>-100.372990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7019 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_rota      nome_usuario       lat2        lon2    elevacao  \\\n",
       "0     11309115  HONDA_CIVIC_2022  25.405903 -100.134304  456.357153   \n",
       "1     11309115  HONDA_CIVIC_2022  25.405721 -100.134209  456.267968   \n",
       "2     11309115  HONDA_CIVIC_2022  25.405539 -100.134116  456.450866   \n",
       "3     11309115  HONDA_CIVIC_2022  25.405356 -100.134023  456.304257   \n",
       "4     11309115  HONDA_CIVIC_2022  25.405174 -100.133930  456.173852   \n",
       "...        ...               ...        ...         ...         ...   \n",
       "7014  11309115  HONDA_CIVIC_2022  25.658215 -100.372543  603.861116   \n",
       "7015  11309115  HONDA_CIVIC_2022  25.658258 -100.372691  603.889631   \n",
       "7016  11309115  HONDA_CIVIC_2022  25.658302 -100.372841  604.039239   \n",
       "7017  11309115  HONDA_CIVIC_2022  25.658348 -100.372990  604.363518   \n",
       "7018  11309115  HONDA_CIVIC_2022  25.658391 -100.373137  604.472864   \n",
       "\n",
       "            data      hora  cidade  estado pais     data_carga_banco  \\\n",
       "0     2024-03-18  16:17:31     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "1     2024-03-18  16:17:32     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "2     2024-03-18  16:17:33     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "3     2024-03-18  16:17:34     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "4     2024-03-18  16:17:35     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "...          ...       ...     ...     ...  ...                  ...   \n",
       "7014  2024-03-18  21:06:54     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7015  2024-03-18  21:06:55     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7016  2024-03-18  21:06:56     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7017  2024-03-18  21:06:57     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7018  2024-03-18  21:06:58     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "\n",
       "           lat1        lon1  \n",
       "0           NaN         NaN  \n",
       "1     25.405903 -100.134304  \n",
       "2     25.405721 -100.134209  \n",
       "3     25.405539 -100.134116  \n",
       "4     25.405356 -100.134023  \n",
       "...         ...         ...  \n",
       "7014  25.658173 -100.372397  \n",
       "7015  25.658215 -100.372543  \n",
       "7016  25.658258 -100.372691  \n",
       "7017  25.658302 -100.372841  \n",
       "7018  25.658348 -100.372990  \n",
       "\n",
       "[7019 rows x 13 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean, cityblock, minkowski\n",
    "\n",
    "df = pd.read_csv('11309115__routes__HONDA_CIVIC_2022.csv', sep=';')\n",
    "\n",
    "df['lat1'] = df['latitude'].shift() #--> Aqui eu passo a função shift na coluna latitude e já crio uma nova coluna chamada lat 1\n",
    "\n",
    "df['lon1'] = df['longitude'].shift() #--> Aqui eu passo a função shift na coluna longitude e já crio uma nova coluna chamada lon 1\n",
    "\n",
    "df = df.rename(columns={'latitude': 'lat2', 'longitude': 'lon2'})\n",
    "\n",
    "df\n",
    "\n",
    "# Observe que a primeira linha fica com valores vazios/nan pq não existe nenhum valor acima do 38.473332\t144.927545\n",
    "# essa função .shift() pega o resultado de uma coluna de uma linha e joga pra uma uma linha abaixo. Nesse exemplo eu criei 2 colunas novas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_rota</th>\n",
       "      <th>nome_usuario</th>\n",
       "      <th>lat2</th>\n",
       "      <th>lon2</th>\n",
       "      <th>elevacao</th>\n",
       "      <th>data</th>\n",
       "      <th>hora</th>\n",
       "      <th>cidade</th>\n",
       "      <th>estado</th>\n",
       "      <th>pais</th>\n",
       "      <th>data_carga_banco</th>\n",
       "      <th>lat1</th>\n",
       "      <th>lon1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405721</td>\n",
       "      <td>-100.134209</td>\n",
       "      <td>456.267968</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405903</td>\n",
       "      <td>-100.134304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405539</td>\n",
       "      <td>-100.134116</td>\n",
       "      <td>456.450866</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405721</td>\n",
       "      <td>-100.134209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405356</td>\n",
       "      <td>-100.134023</td>\n",
       "      <td>456.304257</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405539</td>\n",
       "      <td>-100.134116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.405174</td>\n",
       "      <td>-100.133930</td>\n",
       "      <td>456.173852</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405356</td>\n",
       "      <td>-100.134023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.404990</td>\n",
       "      <td>-100.133838</td>\n",
       "      <td>455.926006</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>16:17:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.405174</td>\n",
       "      <td>-100.133930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658215</td>\n",
       "      <td>-100.372543</td>\n",
       "      <td>603.861116</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658173</td>\n",
       "      <td>-100.372397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658258</td>\n",
       "      <td>-100.372691</td>\n",
       "      <td>603.889631</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658215</td>\n",
       "      <td>-100.372543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658302</td>\n",
       "      <td>-100.372841</td>\n",
       "      <td>604.039239</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658258</td>\n",
       "      <td>-100.372691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658348</td>\n",
       "      <td>-100.372990</td>\n",
       "      <td>604.363518</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658302</td>\n",
       "      <td>-100.372841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018</th>\n",
       "      <td>11309115</td>\n",
       "      <td>HONDA_CIVIC_2022</td>\n",
       "      <td>25.658391</td>\n",
       "      <td>-100.373137</td>\n",
       "      <td>604.472864</td>\n",
       "      <td>2024-03-18</td>\n",
       "      <td>21:06:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MX-NLE</td>\n",
       "      <td>MX</td>\n",
       "      <td>2024-03-19 21:54:02</td>\n",
       "      <td>25.658348</td>\n",
       "      <td>-100.372990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7018 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_rota      nome_usuario       lat2        lon2    elevacao  \\\n",
       "1     11309115  HONDA_CIVIC_2022  25.405721 -100.134209  456.267968   \n",
       "2     11309115  HONDA_CIVIC_2022  25.405539 -100.134116  456.450866   \n",
       "3     11309115  HONDA_CIVIC_2022  25.405356 -100.134023  456.304257   \n",
       "4     11309115  HONDA_CIVIC_2022  25.405174 -100.133930  456.173852   \n",
       "5     11309115  HONDA_CIVIC_2022  25.404990 -100.133838  455.926006   \n",
       "...        ...               ...        ...         ...         ...   \n",
       "7014  11309115  HONDA_CIVIC_2022  25.658215 -100.372543  603.861116   \n",
       "7015  11309115  HONDA_CIVIC_2022  25.658258 -100.372691  603.889631   \n",
       "7016  11309115  HONDA_CIVIC_2022  25.658302 -100.372841  604.039239   \n",
       "7017  11309115  HONDA_CIVIC_2022  25.658348 -100.372990  604.363518   \n",
       "7018  11309115  HONDA_CIVIC_2022  25.658391 -100.373137  604.472864   \n",
       "\n",
       "            data      hora  cidade  estado pais     data_carga_banco  \\\n",
       "1     2024-03-18  16:17:32     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "2     2024-03-18  16:17:33     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "3     2024-03-18  16:17:34     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "4     2024-03-18  16:17:35     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "5     2024-03-18  16:17:36     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "...          ...       ...     ...     ...  ...                  ...   \n",
       "7014  2024-03-18  21:06:54     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7015  2024-03-18  21:06:55     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7016  2024-03-18  21:06:56     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7017  2024-03-18  21:06:57     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "7018  2024-03-18  21:06:58     NaN  MX-NLE   MX  2024-03-19 21:54:02   \n",
       "\n",
       "           lat1        lon1  \n",
       "1     25.405903 -100.134304  \n",
       "2     25.405721 -100.134209  \n",
       "3     25.405539 -100.134116  \n",
       "4     25.405356 -100.134023  \n",
       "5     25.405174 -100.133930  \n",
       "...         ...         ...  \n",
       "7014  25.658173 -100.372397  \n",
       "7015  25.658215 -100.372543  \n",
       "7016  25.658258 -100.372691  \n",
       "7017  25.658302 -100.372841  \n",
       "7018  25.658348 -100.372990  \n",
       "\n",
       "[7018 rows x 13 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['lat1', 'lon1'])\n",
    "df\n",
    "# Aqui eu dropo os valores nulos, ou seja puxo o resultado da linha 2 pra linha 1 nas colunas lat1 e lon1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_euclidiana(row):\n",
    "    coord1 = (float(row['lon1']), float(row['lat1']))\n",
    "    coord2 = (float(row['lon2']), float(row['lat2']))\n",
    "    return euclidean(coord1, coord2)*1000\n",
    "\n",
    "def calc_dist_manhattan(row):\n",
    "    coord1 = (float(row['lon1']), float(row['lat1']))\n",
    "    coord2 = (float(row['lon2']), float(row['lat2']))\n",
    "    return cityblock(coord1, coord2)*1000\n",
    "\n",
    "def calc_dist_minkowski(row, p=3):\n",
    "    coord1 = (float(row['lon1']), float(row['lat1']))\n",
    "    coord2 = (float(row['lon2']), float(row['lat2']))\n",
    "    return minkowski(coord1, coord2, p)*1000\n",
    "\n",
    "# def haversine_distance(row):\n",
    "#     lat1 = float(row['lat1'])\n",
    "#     lon1 = float(row['lon1'])\n",
    "#     lat2 = float(row['lat2'])\n",
    "#     lon2 = float(row['lon2'])\n",
    "    \n",
    "#     if not (-90 <= lat1 <= 90) or not (-90 <= lat2 <= 90):\n",
    "#         raise ValueError(\"Latitude must be in the [-90; 90] range.\")\n",
    "    \n",
    "#     coord1 = (lon1, lat1)\n",
    "#     coord2 = (lon2, lat2)\n",
    "#     return geodesic(coord1, coord2).kilometers\n",
    "\n",
    "\n",
    "\n",
    "# def haversine(lat1, lon1, lat2, lon2):\n",
    "#     R = 6371  # Raio da Terra em km\n",
    "#     d_lat = radians(lat2 - lat1)\n",
    "#     d_lon = radians(lon2 - lon1)\n",
    "#     a = sin(d_lat / 2) ** 2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(d_lon / 2) ** 2\n",
    "#     c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "#     distance = R * c\n",
    "#     return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4140/18453585.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dist_euclidiana'] = df.apply(calc_dist_euclidiana, axis=1)\n",
      "/tmp/ipykernel_4140/18453585.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dist_manhattan'] = df.apply(calc_dist_manhattan, axis=1)\n",
      "/tmp/ipykernel_4140/18453585.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dist_minkowski'] = df.apply(calc_dist_minkowski, axis=1)\n"
     ]
    }
   ],
   "source": [
    "df['dist_euclidiana'] = df.apply(calc_dist_euclidiana, axis=1)\n",
    "df['dist_manhattan'] = df.apply(calc_dist_manhattan, axis=1)\n",
    "df['dist_minkowski'] = df.apply(calc_dist_minkowski, axis=1)\n",
    "#df['dist_haversine'] = df.apply(lambda row: haversine(row['lat1'], row['lon1'], row['lat2'], row['lon2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_rota</th>\n",
       "      <th>nome_usuario</th>\n",
       "      <th>lat2</th>\n",
       "      <th>lon2</th>\n",
       "      <th>elevacao</th>\n",
       "      <th>data</th>\n",
       "      <th>hora</th>\n",
       "      <th>cidade</th>\n",
       "      <th>estado</th>\n",
       "      <th>pais</th>\n",
       "      <th>data_carga_banco</th>\n",
       "      <th>lat1</th>\n",
       "      <th>lon1</th>\n",
       "      <th>dist_euclidiana</th>\n",
       "      <th>dist_manhattan</th>\n",
       "      <th>dist_minkowski</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473334</td>\n",
       "      <td>144.927550</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:26</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473332</td>\n",
       "      <td>144.927545</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473336</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:27</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473334</td>\n",
       "      <td>144.927550</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473333</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:28</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473336</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473344</td>\n",
       "      <td>144.927568</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:30</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473333</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473353</td>\n",
       "      <td>144.927583</td>\n",
       "      <td>93.3</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:31</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473344</td>\n",
       "      <td>144.927568</td>\n",
       "      <td>0.017493</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7647</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477056</td>\n",
       "      <td>144.893626</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:50</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477057</td>\n",
       "      <td>144.893619</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7648</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477054</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:51</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477056</td>\n",
       "      <td>144.893626</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7649</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:52</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477054</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7650</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893627</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:53</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7651</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477052</td>\n",
       "      <td>144.893625</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:54</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893627</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7651 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_rota          nome_usuario       lat2        lon2  elevacao  \\\n",
       "1     11190523  HYUNDAI_IONIQ_5_2022 -38.473334  144.927550      93.2   \n",
       "2     11190523  HYUNDAI_IONIQ_5_2022 -38.473336  144.927554      93.2   \n",
       "3     11190523  HYUNDAI_IONIQ_5_2022 -38.473333  144.927554      93.2   \n",
       "4     11190523  HYUNDAI_IONIQ_5_2022 -38.473344  144.927568      93.2   \n",
       "5     11190523  HYUNDAI_IONIQ_5_2022 -38.473353  144.927583      93.3   \n",
       "...        ...                   ...        ...         ...       ...   \n",
       "7647  11190523  HYUNDAI_IONIQ_5_2022 -38.477056  144.893626     126.8   \n",
       "7648  11190523  HYUNDAI_IONIQ_5_2022 -38.477054  144.893628     126.8   \n",
       "7649  11190523  HYUNDAI_IONIQ_5_2022 -38.477053  144.893628     126.8   \n",
       "7650  11190523  HYUNDAI_IONIQ_5_2022 -38.477053  144.893627     126.8   \n",
       "7651  11190523  HYUNDAI_IONIQ_5_2022 -38.477052  144.893625     126.8   \n",
       "\n",
       "            data      hora     cidade  estado pais     data_carga_banco  \\\n",
       "1     2023-11-21  03:58:26  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "2     2023-11-21  03:58:27  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "3     2023-11-21  03:58:28  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "4     2023-11-21  03:58:30  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "5     2023-11-21  03:58:31  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "...          ...       ...        ...     ...  ...                  ...   \n",
       "7647  2023-11-21  06:25:50  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7648  2023-11-21  06:25:51  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7649  2023-11-21  06:25:52  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7650  2023-11-21  06:25:53  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7651  2023-11-21  06:25:54  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "\n",
       "           lat1        lon1  dist_euclidiana  dist_manhattan  dist_minkowski  \n",
       "1    -38.473332  144.927545         0.005385           0.007        0.005104  \n",
       "2    -38.473334  144.927550         0.004472           0.006        0.004160  \n",
       "3    -38.473336  144.927554         0.003000           0.003        0.003000  \n",
       "4    -38.473333  144.927554         0.017804           0.025        0.015973  \n",
       "5    -38.473344  144.927568         0.017493           0.024        0.016010  \n",
       "...         ...         ...              ...             ...             ...  \n",
       "7647 -38.477057  144.893619         0.007071           0.008        0.007007  \n",
       "7648 -38.477056  144.893626         0.002828           0.004        0.002520  \n",
       "7649 -38.477054  144.893628         0.001000           0.001        0.001000  \n",
       "7650 -38.477053  144.893628         0.001000           0.001        0.001000  \n",
       "7651 -38.477053  144.893627         0.002236           0.003        0.002080  \n",
       "\n",
       "[7651 rows x 16 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_rota</th>\n",
       "      <th>nome_usuario</th>\n",
       "      <th>lat2</th>\n",
       "      <th>lon2</th>\n",
       "      <th>elevacao</th>\n",
       "      <th>data</th>\n",
       "      <th>hora</th>\n",
       "      <th>cidade</th>\n",
       "      <th>estado</th>\n",
       "      <th>pais</th>\n",
       "      <th>data_carga_banco</th>\n",
       "      <th>lat1</th>\n",
       "      <th>lon1</th>\n",
       "      <th>dist_euclidiana</th>\n",
       "      <th>dist_manhattan</th>\n",
       "      <th>dist_minkowski</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473334</td>\n",
       "      <td>144.927550</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:26</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473332</td>\n",
       "      <td>144.927545</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473336</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:27</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473334</td>\n",
       "      <td>144.927550</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473333</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:28</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473336</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473344</td>\n",
       "      <td>144.927568</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:30</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473333</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473353</td>\n",
       "      <td>144.927583</td>\n",
       "      <td>93.3</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:31</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473344</td>\n",
       "      <td>144.927568</td>\n",
       "      <td>0.017493</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_rota          nome_usuario       lat2        lon2  elevacao  \\\n",
       "1  11190523  HYUNDAI_IONIQ_5_2022 -38.473334  144.927550      93.2   \n",
       "2  11190523  HYUNDAI_IONIQ_5_2022 -38.473336  144.927554      93.2   \n",
       "3  11190523  HYUNDAI_IONIQ_5_2022 -38.473333  144.927554      93.2   \n",
       "4  11190523  HYUNDAI_IONIQ_5_2022 -38.473344  144.927568      93.2   \n",
       "5  11190523  HYUNDAI_IONIQ_5_2022 -38.473353  144.927583      93.3   \n",
       "\n",
       "         data      hora     cidade  estado pais     data_carga_banco  \\\n",
       "1  2023-11-21  03:58:26  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "2  2023-11-21  03:58:27  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "3  2023-11-21  03:58:28  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "4  2023-11-21  03:58:30  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "5  2023-11-21  03:58:31  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "\n",
       "        lat1        lon1  dist_euclidiana  dist_manhattan  dist_minkowski  \n",
       "1 -38.473332  144.927545         0.005385           0.007        0.005104  \n",
       "2 -38.473334  144.927550         0.004472           0.006        0.004160  \n",
       "3 -38.473336  144.927554         0.003000           0.003        0.003000  \n",
       "4 -38.473333  144.927554         0.017804           0.025        0.015973  \n",
       "5 -38.473344  144.927568         0.017493           0.024        0.016010  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138329/2347010561.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dist_haversine'] = df.apply(lambda row: haversine(row['lat1'], row['lon1'], row['lat2'], row['lon2']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_rota</th>\n",
       "      <th>nome_usuario</th>\n",
       "      <th>lat2</th>\n",
       "      <th>lon2</th>\n",
       "      <th>elevacao</th>\n",
       "      <th>data</th>\n",
       "      <th>hora</th>\n",
       "      <th>cidade</th>\n",
       "      <th>estado</th>\n",
       "      <th>pais</th>\n",
       "      <th>data_carga_banco</th>\n",
       "      <th>lat1</th>\n",
       "      <th>lon1</th>\n",
       "      <th>dist_euclidiana</th>\n",
       "      <th>dist_manhattan</th>\n",
       "      <th>dist_minkowski</th>\n",
       "      <th>dist_haversine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473334</td>\n",
       "      <td>144.927550</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:26</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473332</td>\n",
       "      <td>144.927545</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473336</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:27</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473334</td>\n",
       "      <td>144.927550</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473333</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:28</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473336</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473344</td>\n",
       "      <td>144.927568</td>\n",
       "      <td>93.2</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:30</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473333</td>\n",
       "      <td>144.927554</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015973</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.473353</td>\n",
       "      <td>144.927583</td>\n",
       "      <td>93.3</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:31</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.473344</td>\n",
       "      <td>144.927568</td>\n",
       "      <td>0.017493</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016010</td>\n",
       "      <td>0.001645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7647</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477056</td>\n",
       "      <td>144.893626</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:50</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477057</td>\n",
       "      <td>144.893619</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7648</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477054</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:51</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477056</td>\n",
       "      <td>144.893626</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7649</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:52</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477054</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7650</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893627</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:53</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893628</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7651</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>-38.477052</td>\n",
       "      <td>144.893625</td>\n",
       "      <td>126.8</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>06:25:54</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU-VIC</td>\n",
       "      <td>AU</td>\n",
       "      <td>2023-11-21 12:18:07</td>\n",
       "      <td>-38.477053</td>\n",
       "      <td>144.893627</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.000207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7651 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_rota          nome_usuario       lat2        lon2  elevacao  \\\n",
       "1     11190523  HYUNDAI_IONIQ_5_2022 -38.473334  144.927550      93.2   \n",
       "2     11190523  HYUNDAI_IONIQ_5_2022 -38.473336  144.927554      93.2   \n",
       "3     11190523  HYUNDAI_IONIQ_5_2022 -38.473333  144.927554      93.2   \n",
       "4     11190523  HYUNDAI_IONIQ_5_2022 -38.473344  144.927568      93.2   \n",
       "5     11190523  HYUNDAI_IONIQ_5_2022 -38.473353  144.927583      93.3   \n",
       "...        ...                   ...        ...         ...       ...   \n",
       "7647  11190523  HYUNDAI_IONIQ_5_2022 -38.477056  144.893626     126.8   \n",
       "7648  11190523  HYUNDAI_IONIQ_5_2022 -38.477054  144.893628     126.8   \n",
       "7649  11190523  HYUNDAI_IONIQ_5_2022 -38.477053  144.893628     126.8   \n",
       "7650  11190523  HYUNDAI_IONIQ_5_2022 -38.477053  144.893627     126.8   \n",
       "7651  11190523  HYUNDAI_IONIQ_5_2022 -38.477052  144.893625     126.8   \n",
       "\n",
       "            data      hora     cidade  estado pais     data_carga_banco  \\\n",
       "1     2023-11-21  03:58:26  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "2     2023-11-21  03:58:27  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "3     2023-11-21  03:58:28  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "4     2023-11-21  03:58:30  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "5     2023-11-21  03:58:31  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "...          ...       ...        ...     ...  ...                  ...   \n",
       "7647  2023-11-21  06:25:50  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7648  2023-11-21  06:25:51  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7649  2023-11-21  06:25:52  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7650  2023-11-21  06:25:53  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "7651  2023-11-21  06:25:54  Melbourne  AU-VIC   AU  2023-11-21 12:18:07   \n",
       "\n",
       "           lat1        lon1  dist_euclidiana  dist_manhattan  dist_minkowski  \\\n",
       "1    -38.473332  144.927545         0.005385           0.007        0.005104   \n",
       "2    -38.473334  144.927550         0.004472           0.006        0.004160   \n",
       "3    -38.473336  144.927554         0.003000           0.003        0.003000   \n",
       "4    -38.473333  144.927554         0.017804           0.025        0.015973   \n",
       "5    -38.473344  144.927568         0.017493           0.024        0.016010   \n",
       "...         ...         ...              ...             ...             ...   \n",
       "7647 -38.477057  144.893619         0.007071           0.008        0.007007   \n",
       "7648 -38.477056  144.893626         0.002828           0.004        0.002520   \n",
       "7649 -38.477054  144.893628         0.001000           0.001        0.001000   \n",
       "7650 -38.477053  144.893628         0.001000           0.001        0.001000   \n",
       "7651 -38.477053  144.893627         0.002236           0.003        0.002080   \n",
       "\n",
       "      dist_haversine  \n",
       "1           0.000489  \n",
       "2           0.000413  \n",
       "3           0.000334  \n",
       "4           0.001727  \n",
       "5           0.001645  \n",
       "...              ...  \n",
       "7647        0.000619  \n",
       "7648        0.000282  \n",
       "7649        0.000111  \n",
       "7650        0.000087  \n",
       "7651        0.000207  \n",
       "\n",
       "[7651 rows x 17 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_rota = df['id_rota'].iloc[0]\n",
    "nome_usuario = df['nome_usuario'].iloc[0]\n",
    "data = df['data'].iloc[0]\n",
    "hora_inicio = df['hora'].iloc[0]\n",
    "hora_fim = df['hora'].iloc[-1]  # Última ocorrência de hora\n",
    "cidade = df['cidade'].iloc[0]\n",
    "pais = df['pais'].iloc[0]\n",
    "\n",
    "# Calcular o tempo de viagem\n",
    "primeira_hora = pd.to_datetime(hora_inicio)\n",
    "ultima_hora = pd.to_datetime(hora_fim)\n",
    "if ultima_hora > primeira_hora:\n",
    "    tempo_viagem = ultima_hora - primeira_hora\n",
    "else:\n",
    "    tempo_viagem = primeira_hora - ultima_hora\n",
    "\n",
    "# Calcular as somas das distâncias\n",
    "dist_euclidiana = df['dist_euclidiana'].sum()\n",
    "dist_manhattan = df['dist_manhattan'].sum()\n",
    "dist_minkowski = df['dist_minkowski'].sum()\n",
    "\n",
    "# Definir a data de carga do banco como a hora atual\n",
    "data_carga_banco = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distancias = pd.DataFrame({\n",
    "    'id_rota': [id_rota],\n",
    "    'nome_usuario': [nome_usuario],\n",
    "    'data': [data],\n",
    "    'hora_inicio': [hora_inicio],\n",
    "    'hora_fim': [hora_fim],\n",
    "    'tempo_viagem': [tempo_viagem],\n",
    "    'cidade': [cidade],\n",
    "    'pais': [pais],\n",
    "    'dist_euclidiana': [dist_euclidiana],\n",
    "    'dist_manhattan': [dist_manhattan],\n",
    "    'dist_minkowski': [dist_minkowski],\n",
    "    'data_carga_banco': [data_carga_banco]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_rota</th>\n",
       "      <th>nome_usuario</th>\n",
       "      <th>data</th>\n",
       "      <th>hora_inicio</th>\n",
       "      <th>hora_fim</th>\n",
       "      <th>tempo_viagem</th>\n",
       "      <th>cidade</th>\n",
       "      <th>pais</th>\n",
       "      <th>dist_euclidiana</th>\n",
       "      <th>dist_manhattan</th>\n",
       "      <th>dist_minkowski</th>\n",
       "      <th>data_carga_banco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11190523</td>\n",
       "      <td>HYUNDAI_IONIQ_5_2022</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>03:58:26</td>\n",
       "      <td>06:25:54</td>\n",
       "      <td>0 days 02:27:28</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>AU</td>\n",
       "      <td>118.985753</td>\n",
       "      <td>150.16</td>\n",
       "      <td>112.823311</td>\n",
       "      <td>2024-03-19 20:49:59.612378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_rota          nome_usuario        data hora_inicio  hora_fim  \\\n",
       "0  11190523  HYUNDAI_IONIQ_5_2022  2023-11-21    03:58:26  06:25:54   \n",
       "\n",
       "     tempo_viagem     cidade pais  dist_euclidiana  dist_manhattan  \\\n",
       "0 0 days 02:27:28  Melbourne   AU       118.985753          150.16   \n",
       "\n",
       "   dist_minkowski           data_carga_banco  \n",
       "0      112.823311 2024-03-19 20:49:59.612378  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distancias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4ª Função - Enriquecendo o DF com informação de pais - estado e cidade - Carregando o DF Enriquecido na camada GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install minio\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "CAMADA_SILVER = 'silver'\n",
    "CAMADA_GOLD = 'gold'\n",
    "\n",
    "\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_SILVER) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "\n",
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:\n",
    "        \n",
    "#    obj_rota_csv = minioclient.get_object(CAMADA_SILVER, arquivo_rotas_gpx_csv.object_name)        \n",
    "#    csv_decod = obj_rota_csv.data.decode('utf-8')  # Convertendo bytes para string    \n",
    "#    arquivo_csv = StringIO(csv_decod)\n",
    "#    df = pd.read_csv(arquivo_csv, sep=';')\n",
    "\n",
    "#    batch_size = 1000\n",
    "#    batch_list = [df[i:i+batch_size].copy() for i in range(0, len(df), batch_size)]\n",
    "#    geolocator = Nominatim(user_agent=\"geoapiExercises\", timeout=10)\n",
    "#    processed_batches = []  # Inicialize a lista de lotes processados\n",
    "#    last_locations = {}\n",
    "\n",
    "#    for batch_df in batch_list:\n",
    "#        # Obtenha a primeira e a última linha do lote\n",
    "#        first_row = batch_df.iloc[0]\n",
    "#        last_row = batch_df.iloc[-1]\n",
    "\n",
    "#        first_location_str = f\"{first_row['latitude']},{first_row['longitude']}\"\n",
    "#        last_location_str = f\"{last_row['latitude']},{last_row['longitude']}\"\n",
    "\n",
    "#        # Verifique se a primeira e a última localização são iguais à última localização verificada\n",
    "#        if are_locations_equal(first_location_str, last_location_str) and are_locations_equal(first_location_str, last_location):\n",
    "#            # Se forem iguais, não é necessário consultar o serviço novamente\n",
    "#            location = last_location\n",
    "#        else:\n",
    "#            # Se forem diferentes, consulte o serviço de geocodificação para a última localização\n",
    "#            location = geolocator.reverse(last_location_str)\n",
    "#            last_location = location\n",
    "\n",
    "#        address = location.raw['address']\n",
    "#        cidade = address.get('county') or address.get('city') or address.get('suburb')\n",
    "#        estado = address.get('ISO3166-2-lvl4').split('-')[1]\n",
    "#        pais = address.get('country_code')\n",
    "                \n",
    "#        pais = pais.upper()\n",
    "        \n",
    "#        df.loc[:, 'cidade'] = cidade\n",
    "#        df.loc[:, 'estado'] = estado\n",
    "#        df.loc[:, 'pais'] = pais\n",
    "\n",
    "#    # Converta o DataFrame enriquecido de volta para CSV\n",
    "#    enriched_csv = df.to_csv(index=False, sep=';')\n",
    "#    enriched_csv_bytes = enriched_csv.encode('utf-8')\n",
    "\n",
    "#    # Crie um buffer de bytes\n",
    "#    enriched_csv_buffer = BytesIO(enriched_csv_bytes)\n",
    "\n",
    "#    minioclient.put_object(\n",
    "#        CAMADA_GOLD,\n",
    "#        arquivo_rotas_gpx_csv.object_name,  # Use o mesmo nome de arquivo\n",
    "#        data=enriched_csv_buffer,\n",
    "#        length=len(enriched_csv_bytes),\n",
    "#        content_type='application/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_locations_equal(location1, location2):\n",
    "    return location1 == location2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que 'result' seja o dicionário retornado pela geocodificação\n",
    "address = result.raw['address']\n",
    "\n",
    "cidade = None\n",
    "estado = None\n",
    "pais = None\n",
    "\n",
    "# Mapeie os atributos de endereço para os nomes padronizados\n",
    "for key, aliases in address_mappings.items():\n",
    "    for alias in aliases:\n",
    "        if alias in address:\n",
    "            if key == 'city' or key == 'county':\n",
    "                cidade = address[alias]                            \n",
    "            \n",
    "\n",
    "# Agora você tem 'cidade', 'estado' e 'pais' padronizados independentemente do formato de endereço\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_SILVER) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:    \n",
    "    obj_rota_csv = minioclient.get_object(CAMADA_SILVER, arquivo_rotas_gpx_csv.object_name)        \n",
    "    csv_decod = obj_rota_csv.data.decode('utf-8')  # Convertendo bytes para string    \n",
    "    arquivo_csv = StringIO(csv_decod)\n",
    "    df = pd.read_csv(arquivo_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'/home/thiago/tcc_ufrj/PRE_PROCESSAMENTO/10125901__Routes_from_sunnypilot_0_9_4_1-release__KIA_EV6_2022__.csv', sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "Latitude = \"40.82486026618991\"\n",
    "Longitude = \"-87.99828066563114\"\n",
    "location = geolocator.reverse(f'{Latitude},{Longitude}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cidade = address.get('county') or address.get('city') or address.get('suburb')\n",
    "estado = address.get('ISO3166-2-lvl4').split('-')[1]\n",
    "pais = address.get('country_code')\n",
    "pais\n",
    "\n",
    "\n",
    "cidade = address.get('county') or address.get('city') or address.get('suburb')\n",
    "estado = address.get('ISO3166-2-lvl4').split('-')[1]\n",
    "pais = address.get('country_code')\n",
    "pais\n",
    "\n",
    "\n",
    "address = location.raw['address']\n",
    "cidade = address.get('suburb')\n",
    "estado = address.get('ISO3166-2-lvl4')\n",
    "pais = address.get('country_code')\n",
    "\n",
    "estado = estado[-2:]\n",
    "pais = pais.upper()\n",
    "\n",
    "print(f\"Cidade: {cidade}\")\n",
    "print(f\"Estado (abreviado): {estado[-2:]}\")\n",
    "print(f\"País: {pais}\")\n",
    "\n",
    "\n",
    "\n",
    "address = location.raw['address']\n",
    "cidade = address.get('county')\n",
    "estado = address.get('ISO3166-2-lvl4')\n",
    "pais = address.get('country_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5ª Função - Lendo o bucket Silver e Escrevendo os arquivos no banco de dados Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "### IMPORTAÇÃO DAS BIBLIOTECAS NECESSÁRIAS ###\n",
    "##############################################\n",
    "# pip install minio\n",
    "# pip install psycopg2\n",
    "\n",
    "import psycopg2\n",
    "from minio import Minio\n",
    "from io import StringIO, BytesIO\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "####################################\n",
    "### DEFINIÇÃO DA CAMADA NO MINIO ###\n",
    "####################################\n",
    "CAMADA_GOLD = 'gold'\n",
    "CAMADA_FILES_IN_TABLE = 'files-in-table'\n",
    "\n",
    "##############################################\n",
    "### CRIANDO UMA INSTÂNCIA DO CLIENTE MINIO ###\n",
    "##############################################\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)\n",
    "\n",
    "#############################################\n",
    "### ACESSANDO O BANCO DE DADOS POSTGRESQL ###\n",
    "#############################################\n",
    "db_config = {\n",
    "'host': 'localhost',\n",
    "'database': 'postgres',\n",
    "'user': 'postgres',\n",
    "'password': 'postgres',\n",
    "}\n",
    "\n",
    "########################################\n",
    "### COMANDO SQL PARA A OPERAÇÃO COPY ###\n",
    "########################################\n",
    "copy_sql = \"\"\"\n",
    "    COPY tb_gpx_full (id_rota, nome_usuario, latitude, longitude, elevacao, data_rota, hora_rota, cidade, estado, pais)\n",
    "    FROM stdin WITH CSV HEADER DELIMITER as ';'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Lista todos os arquivos na camada \"gold\" do Minio que têm extensão .csv\n",
    "    arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_GOLD) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "    \n",
    "    # Verifica se há arquivos no bucket antes de continuar\n",
    "    if not arquivos_rotas_gpx_csv:\n",
    "        print(\"Não existem arquivos CSV no bucket. Nenhuma carga de dados será executada.\")\n",
    "\n",
    "    else:\n",
    "        # Conexão com o banco de dados PostgreSQL\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Itera sobre cada arquivo CSV encontrado no Minio\n",
    "        for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:\n",
    "            # Obtém o objeto do arquivo CSV do Minio  \n",
    "            obj_rota_csv = minioclient.get_object(CAMADA_GOLD, arquivo_rotas_gpx_csv.object_name)            \n",
    "\n",
    "            # Decodifica os dados do arquivo CSV de bytes para string\n",
    "            csv_decod = obj_rota_csv.data.decode('utf-8')  # Convertendo bytes para string\n",
    "            arquivo_csv = StringIO(csv_decod)\n",
    "            df = pd.read_csv(arquivo_csv, sep=';')\n",
    "            csv_bytes = df.to_csv(index=False,sep=';').encode('utf-8')\n",
    "            csv_buffer = BytesIO(csv_bytes)\n",
    "            nome_arquivo = arquivo_rotas_gpx_csv.object_name\n",
    "\n",
    "\n",
    "            # Usa io.StringIO para criar um objeto de arquivo legível a partir da string CSV\n",
    "            with io.StringIO(csv_decod) as file:        \n",
    "\n",
    "                # Executa o comando COPY para inserir os dados no banco de dados PostgreSQL\n",
    "                cursor.copy_expert(sql=copy_sql, file=file)\n",
    "\n",
    "            ## Commit para salvar as alterações no banco de dados    \n",
    "            conn.commit()\n",
    "\n",
    "\n",
    "            minioclient.put_object( #-->Usdndo o metodo do MinIO responsável por adicionar arquivos no Bucket\n",
    "                    CAMADA_FILES_IN_TABLE, #--> Nome da camada de destino do arquivo transformado\n",
    "                    nome_arquivo, #--> Nome do arquivo a ser adicionado na nova camada\n",
    "                    data=csv_buffer, #--> Objeto csv_buffer que contém os bytes do arquivo CSV.\n",
    "                    length=len(csv_bytes), #--> Especificando o comprimento dos bytes do arquivo CSV que você está enviando.\n",
    "                    content_type='application/csv')        \n",
    "\n",
    "            \n",
    "            # Após a copia para o bucket de segurança os arquivos são eliminados da camada gold\n",
    "            minioclient.remove_object(CAMADA_GOLD, arquivo_rotas_gpx_csv.object_name) \n",
    "\n",
    "        # Fecha a conexão com o banco de dados PostgreSQL\n",
    "        conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    # Em caso de erro, imprime a mensagem de erro\n",
    "    print(f\"Erro: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "data_carga_banco = datetime.now().strftime('%Y-%m-%d')\n",
    "print(data_carga_banco)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista todos os arquivos na camada \"gold\" do Minio que têm extensão .csv\n",
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_GOLD) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "\n",
    "# Verifica se há arquivos no bucket antes de continuar\n",
    "if not arquivos_rotas_gpx_csv:\n",
    "    print(\"Não existem arquivos CSV no bucket. Nenhuma carga de dados será executada.\")\n",
    "else:\n",
    "    # Conexão com o banco de dados PostgreSQL\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "    # Itera sobre cada arquivo CSV encontrado no Minio\n",
    "    for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv:\n",
    "        # Obtém o objeto do arquivo CSV do Minio  \n",
    "        obj_rota_csv = minioclient.get_object(CAMADA_GOLD, arquivo_rotas_gpx_csv.object_name)            \n",
    "\n",
    "        # Decodifica os dados do arquivo CSV de bytes para string\n",
    "        csv_decod = obj_rota_csv.data.decode('utf-8')  # Convertendo bytes para string\n",
    "        arquivo_csv = StringIO(csv_decod)\n",
    "        df = pd.read_csv(arquivo_csv, sep=';')\n",
    "        csv_bytes = df.to_csv(index=False,sep=';').encode('utf-8')\n",
    "        csv_buffer = BytesIO(csv_bytes)            \n",
    "        nome_arquivo = arquivo_rotas_gpx_csv.object_name\n",
    "\n",
    "        minioclient.put_object( #-->Usdndo o metodo do MinIO responsável por adicionar arquivos no Bucket\n",
    "                    CAMADA_FILES_IN_TABLE, #--> Nome da camada de destino do arquivo transformado\n",
    "                    nome_arquivo, #--> Nome do arquivo a ser adicionado na nova camada\n",
    "                    data=csv_buffer, #--> Objeto csv_buffer que contém os bytes do arquivo CSV.\n",
    "                    length=len(csv_bytes), #--> Especificando o comprimento dos bytes do arquivo CSV que você está enviando.\n",
    "                    content_type='application/csv')        \n",
    "\n",
    "        minioclient.remove_object(CAMADA_GOLD, arquivo_rotas_gpx_csv.object_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from minio import Minio\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import euclidean, cityblock, minkowski\n",
    "\n",
    "CAMADA_GOLD = 'gold'\n",
    "DISTANCIAS = 'distancias'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "### CRIANDO UMA INSTÂNCIA DO CLIENTE MINIO ###\n",
    "##############################################\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)\n",
    "\n",
    "#############################################\n",
    "### ACESSANDO O BANCO DE DADOS POSTGRESQL ###\n",
    "#############################################\n",
    "db_config = {\n",
    "'host': 'localhost',\n",
    "'database': 'postgres',\n",
    "'user': 'postgres',\n",
    "'password': 'postgres',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_euclidiana(row):\n",
    "    coord1 = (float(row['lon1']), float(row['lat1']))\n",
    "    coord2 = (float(row['lon2']), float(row['lat2']))\n",
    "    return euclidean(coord1, coord2) * 1000\n",
    "\n",
    "def calc_dist_manhattan(row):\n",
    "    coord1 = (float(row['lon1']), float(row['lat1']))\n",
    "    coord2 = (float(row['lon2']), float(row['lat2']))\n",
    "    return cityblock(coord1, coord2)*1000\n",
    "\n",
    "def calc_dist_minkowski(row, p=3):\n",
    "    coord1 = (float(row['lon1']), float(row['lat1']))\n",
    "    coord2 = (float(row['lon2']), float(row['lat2']))\n",
    "    return minkowski(coord1, coord2, p)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_GOLD) if arquivo_gpx.object_name.endswith(\".csv\")] #--> Listando todos os arquivos da camada bronze do datalake com extensão .csv\n",
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv: #--> Iterando sobre a lista encontrada\n",
    "    obj_rota_csv = minioclient.get_object(CAMADA_GOLD, arquivo_rotas_gpx_csv.object_name) #--> Obtendo o nome do arquivo de dentro da camada bronze\n",
    "    csv_decod = obj_rota_csv.data.decode('utf-8') #--> Decodificando o arquivo encontrado para utf-8 - Essa conversão transforma os dados obtidos do arquivo no bucket em bytes\n",
    "    arquivo_csv = StringIO(csv_decod) #--> Convertendo os bytes em string\n",
    "    df = pd.read_csv(arquivo_csv, sep=';') #--> Convertendo string para pandas dataframe\n",
    "    df['lat1'] = df['latitude'].shift() #--> Aqui eu passo a função shift na coluna latitude e já crio uma nova coluna chamada lat 1\n",
    "    df['lon1'] = df['longitude'].shift() #--> Aqui eu passo a função shift na coluna longitude e já crio uma nova coluna chamada lon 1\n",
    "    df = df.rename(columns={'latitude': 'lat2', 'longitude': 'lon2'})\n",
    "    df = df.dropna()\n",
    "\n",
    "    df['dist_euclidiana'] = df.apply(calc_dist_euclidiana, axis=1)\n",
    "    df['dist_manhattan'] = df.apply(calc_dist_manhattan, axis=1)\n",
    "    df['dist_minkowski'] = df.apply(calc_dist_minkowski, axis=1)\n",
    "\n",
    "\n",
    "#     # Obtendo as informações isoladas\n",
    "#     id_rota = df['id_rota'].iloc[0]\n",
    "#     nome_usuario = df['nome_usuario'].iloc[0]\n",
    "#     data = df['data'].iloc[0]\n",
    "#     hora_inicio = df['hora'].iloc[0]\n",
    "#     hora_fim = df['hora'].iloc[-1]  # Última ocorrência de hora\n",
    "#     cidade = df['cidade'].iloc[0]\n",
    "#     pais = df['pais'].iloc[0]\n",
    "#     # Calcular o tempo de viagem\n",
    "#     primeira_hora = pd.to_datetime(hora_inicio)\n",
    "#     ultima_hora = pd.to_datetime(hora_fim)\n",
    "#     if ultima_hora > primeira_hora:\n",
    "#         tempo_viagem = ultima_hora - primeira_hora\n",
    "#     else:\n",
    "#         tempo_viagem = primeira_hora - ultima_hora\n",
    "#     # Calcular as somas das distâncias\n",
    "#     dist_euclidiana = df['dist_euclidiana'].sum()\n",
    "#     dist_manhattan = df['dist_manhattan'].sum()\n",
    "#     dist_minkowski = df['dist_minkowski'].sum()\n",
    "#     # Definir a data de carga do banco como a hora atual\n",
    "#     data_carga_banco = datetime.now()\n",
    "#     # Criando o dataframe\n",
    "#     df_distancias = pd.DataFrame({\n",
    "#         'id_rota': [id_rota],\n",
    "#         'nome_usuario': [nome_usuario],\n",
    "#         'data': [data],\n",
    "#         'hora_inicio': [hora_inicio],\n",
    "#         'hora_fim': [hora_fim],\n",
    "#         'tempo_viagem': [tempo_viagem],\n",
    "#         'cidade': [cidade],\n",
    "#         'pais': [pais],\n",
    "#         'dist_euclidiana': [dist_euclidiana],\n",
    "#         'dist_manhattan': [dist_manhattan],\n",
    "#         'dist_minkowski': [dist_minkowski],\n",
    "#         'data_carga_banco': [data_carga_banco]\n",
    "#     })\n",
    "\n",
    "#     # Converta o DataFrame enriquecido de volta para CSV\n",
    "#     distancias_csv = df_distancias.to_csv(index=False, sep=';')\n",
    "#     distancias_csv_bytes = distancias_csv.encode('utf-8')\n",
    "\n",
    "# # Crie um buffer de bytes\n",
    "#     distancias_csv_buffer = BytesIO(distancias_csv_bytes)\n",
    "\n",
    "#     minioclient.put_object(\n",
    "#         DISTANCIAS,\n",
    "#         arquivo_rotas_gpx_csv.object_name,  # Use o mesmo nome de arquivo\n",
    "#         data=distancias_csv_buffer,\n",
    "#         length=len(distancias_csv_bytes),\n",
    "#         content_type='application/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\"/home/thiago/tcc_ufrj/scripts_finalizados/.env\")\n",
    "\n",
    "CREDENTIAL = config.get(\"TOKEN_API\")\n",
    "type(CREDENTIAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(CREDENTIAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_api = '5b3ce3597851110001cf624838eb860780704eca99c41867a83c9f6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_GOLD) if arquivo_gpx.object_name.endswith(\".csv\")] #--> Listando todos os arquivos da camada bronze do datalake com extensão .csv\n",
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv: #--> Iterando sobre a lista encontrada\n",
    "    obj_rota_csv = minioclient.get_object(CAMADA_GOLD, arquivo_rotas_gpx_csv.object_name) #--> Obtendo o nome do arquivo de dentro da camada bronze\n",
    "    csv_decod = obj_rota_csv.data.decode('utf-8') #--> Decodificando o arquivo encontrado para utf-8 - Essa conversão transforma os dados obtidos do arquivo no bucket em bytes\n",
    "    arquivo_csv = StringIO(csv_decod) #--> Convertendo os bytes em string\n",
    "    df = pd.read_csv(arquivo_csv, sep=';') #--> Convertendo string para pandas dataframe\n",
    "\n",
    "    #df = pd.read_csv('11190523__routes__HYUNDAI_IONIQ_5_2022.csv', sep=';')\n",
    "    df['lat1'] = df['latitude'].shift() #--> Aqui eu passo a função shift na coluna latitude e já crio uma nova coluna chamada lat 1\n",
    "    df['lon1'] = df['longitude'].shift() #--> Aqui eu passo a função shift na coluna longitude e já crio uma nova coluna chamada lon 1\n",
    "    df = df.rename(columns={'latitude': 'lat2', 'longitude': 'lon2'})\n",
    "    df = df.dropna(subset=['lat1', 'lon1'])\n",
    "\n",
    "\n",
    "    df['dist_euclidiana'] = df.apply(calc_dist_euclidiana, axis=1)\n",
    "    df['dist_manhattan'] = df.apply(calc_dist_manhattan, axis=1)\n",
    "    df['dist_minkowski'] = df.apply(calc_dist_minkowski, axis=1)\n",
    "\n",
    "\n",
    "    # Obtendo as informações isoladas\n",
    "    id_rota = df['id_rota'].iloc[0]\n",
    "    nome_usuario = df['nome_usuario'].iloc[0]\n",
    "    data_viagem = df['data'].iloc[0]\n",
    "    hora_inicio = df['hora'].iloc[0]\n",
    "    hora_fim = df['hora'].iloc[-1]  # Última ocorrência de hora\n",
    "    cidade = df['cidade'].iloc[0]\n",
    "    pais = df['pais'].iloc[0]\n",
    "    # Calcular o tempo de viagem\n",
    "    primeira_hora = pd.to_datetime(hora_inicio)\n",
    "    ultima_hora = pd.to_datetime(hora_fim)\n",
    "    if ultima_hora > primeira_hora:\n",
    "        tempo_viagem = ultima_hora - primeira_hora\n",
    "    else:\n",
    "        tempo_viagem = primeira_hora - ultima_hora\n",
    "    # Calcular as somas das distâncias\n",
    "    dist_euclidiana = df['dist_euclidiana'].sum()\n",
    "    dist_manhattan = df['dist_manhattan'].sum()\n",
    "    dist_minkowski = df['dist_minkowski'].sum()\n",
    "    # Definir a data de carga do banco como a hora atual\n",
    "    data_carga_banco = datetime.now()\n",
    "    # Criando o dataframe\n",
    "    df_distancias = pd.DataFrame({\n",
    "        'id_rota': [id_rota],\n",
    "        'nome_usuario': [nome_usuario],\n",
    "        'data_viagem': [data_viagem],\n",
    "        'hora_inicio': [hora_inicio],\n",
    "        'hora_fim': [hora_fim],\n",
    "        'tempo_viagem': [tempo_viagem],\n",
    "        'cidade': [cidade],\n",
    "        'pais': [pais],\n",
    "        'dist_euclidiana': [dist_euclidiana],\n",
    "        'dist_manhattan': [dist_manhattan],\n",
    "        'dist_minkowski': [dist_minkowski],\n",
    "        'data_carga_banco': [data_carga_banco]\n",
    "    })\n",
    "\n",
    "\n",
    "# Converta o DataFrame enriquecido de volta para CSV\n",
    "    distancias_csv = df_distancias.to_csv(index=False, sep=';')\n",
    "    distancias_csv_bytes = distancias_csv.encode('utf-8')\n",
    "\n",
    "# Crie um buffer de bytes\n",
    "    distancias_csv_buffer = BytesIO(distancias_csv_bytes)\n",
    "\n",
    "    minioclient.put_object(\n",
    "        DISTANCIAS,\n",
    "        arquivo_rotas_gpx_csv.object_name,  # Use o mesmo nome de arquivo\n",
    "        data=distancias_csv_buffer,\n",
    "        length=len(distancias_csv_bytes),\n",
    "        content_type='application/csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando o geopy.geocoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openrouteservice \n",
    "# pip install folium \n",
    "# pip install pyrosm\n",
    "\n",
    "# https://openrouteservice.org/\n",
    "\n",
    "import psycopg2\n",
    "from minio import Minio\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "#import openrouteservice\n",
    "#import ast\n",
    "#from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import openrouteservice\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic as GD\n",
    "\n",
    "api = 'api_route'\n",
    "token_api = '5b3ce3597851110001cf624838eb860780704eca99c41867a83c9f6b'\n",
    "\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)\n",
    "\n",
    "\n",
    "db_config = {\n",
    "'host': 'localhost',\n",
    "'database': 'postgres',\n",
    "'user': 'postgres',\n",
    "'password': 'postgres',\n",
    "}\n",
    "\n",
    "CAMADA_GOLD = 'gold'\n",
    "CAMADA_FILES_IN_TABLE = 'files-in-table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_rotas_gpx_csv = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(CAMADA_GOLD) if arquivo_gpx.object_name.endswith(\".csv\")] #--> Listando todos os arquivos da camada bronze do datalake com extensão .csv\n",
    "for arquivo_rotas_gpx_csv in arquivos_rotas_gpx_csv: #--> Iterando sobre a lista encontrada\n",
    "    try:    \n",
    "        obj_rota_csv = minioclient.get_object(CAMADA_GOLD, arquivo_rotas_gpx_csv.object_name) #--> Obtendo o nome do arquivo de dentro da camada bronze\n",
    "        csv_decod = obj_rota_csv.data.decode('utf-8') #--> Decodificando o arquivo encontrado para utf-8 - Essa conversão transforma os dados obtidos do arquivo no bucket em bytes\n",
    "        arquivo_csv = StringIO(csv_decod) #--> Convertendo os bytes em string\n",
    "        df = pd.read_csv(arquivo_csv, sep=';') #--> Convertendo string para pandas dataframe        \n",
    "        \n",
    "        id_rota = df.loc[df.index[0], 'id_rota']\n",
    "        nome_usuario = df.loc[df.index[0], 'nome_usuario']\n",
    "        data_inicio_rota = df.loc[df.index[0], 'data']\n",
    "        data_fim_rota = df.loc[df.index[-1], 'data']\n",
    "        inicio_rota = df.loc[df.index[0], 'hora']\n",
    "        fim_rota = df.loc[df.index[-1], 'hora']\n",
    "        latitude_inicial = df.loc[df.index[0], 'latitude']\n",
    "        longitude_inicial = df.loc[df.index[0], 'longitude']\n",
    "        latitude_final = df.loc[df.index[-1], 'latitude']\n",
    "        longitude_final = df.loc[df.index[-1], 'longitude']\n",
    "        cidade = df.loc[df.index[0], 'cidade']\n",
    "        estado = df.loc[df.index[0], 'estado']\n",
    "        pais = df.loc[df.index[0], 'pais']\n",
    "        id_unico = f'{id_rota}__{data_inicio_rota}__{inicio_rota}__{nome_usuario}'\n",
    "        data_carga_banco = df.loc[df.index[0], 'data_carga_banco']\n",
    "        longitude_inicial_float = float(longitude_inicial)\n",
    "        latitude_inicial_float = float(latitude_inicial)    \n",
    "        longitude_final_float = float(longitude_final)\n",
    "        latitude_final_float = float(latitude_final)    \n",
    "        #body = {\"coordinates\":[[8.681495,49.41461],[8.687872,49.420318]],\"radiuses\":\"-1\"}\n",
    "        body = {\"coordinates\": [[longitude_inicial_float, latitude_inicial_float],[longitude_final_float, latitude_final_float]],\"radiuses\": \"-1\"}\n",
    "\n",
    "        headers = {\n",
    "            'Accept': 'application/json, application/geo+json, application/gpx+xml, img/png; charset=utf-8',\n",
    "            'Authorization': token_api,\n",
    "            'Content-Type': 'application/json; charset=utf-8'\n",
    "        }\n",
    "    \n",
    "        call = requests.post('https://api.openrouteservice.org/v2/directions/driving-car', json=body, headers=headers)\n",
    "        response = eval(call.text)\n",
    "        duration = response['routes'][0]['summary']['duration']\n",
    "        distance = response['routes'][0]['summary']['distance']\n",
    "        distancia_real = distance / 1000\n",
    "\n",
    "#        conn = psycopg2.connect(**db_config)\n",
    "#        cursor = conn.cursor()\n",
    "#        insert = f'''\n",
    "#            insert into tb_distancias_percorridas_api (id_unico,nome_usuario,data_inicio_rota,data_fim_rota,hora_inicio_rota,hora_fim_rota,latitude_inicial,longitude_inicial,latitude_final,longitude_final,cidade,estado,pais,distancia_real_km_api,data_carga_banco)\n",
    "#            values (\n",
    "#                '{id_unico}',\n",
    "#                '{nome_usuario}',\n",
    "#                '{data_inicio_rota}',\n",
    "#                '{data_fim_rota}',\n",
    "#                '{inicio_rota}',\n",
    "#                '{fim_rota}',\n",
    "#                '{latitude_inicial}',\n",
    "#                '{longitude_inicial}',\n",
    "#                '{latitude_final}',\n",
    "#                '{longitude_final}',\n",
    "#                '{cidade}',\n",
    "#                '{estado}',\n",
    "#                '{pais}',              \n",
    "#                '{distancia_real}',\n",
    "#                '{data_carga_banco}'\n",
    "#                )'''\n",
    "#        cursor.execute(insert)\n",
    "#        conn.commit()\n",
    "#        conn.close()\n",
    "#        print(insert)\n",
    "    except Exception as e:        \n",
    "        print(f\"Erro: {str(e)} no id {id_unico}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\"coordinates\": [[0.5630799941718578, 48.37511999532581],[0.5221799947321415, 48.324159951880574]],\"radiuses\": \"-1\"}\n",
    "\n",
    "headers = {\n",
    "            'Accept': 'application/json, application/geo+json, application/gpx+xml, img/png; charset=utf-8',\n",
    "            'Authorization': token_api,\n",
    "            'Content-Type': 'application/json; charset=utf-8'\n",
    "        }\n",
    "    \n",
    "call = requests.post('https://api.openrouteservice.org/v2/directions/driving-car', json=body, headers=headers)\n",
    "response = eval(call.text)\n",
    "\n",
    "#48.37511999532581\t0.5630799941718578\t48.324159951880574\t0.5221799947321415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "consulta_veiculos = \"\"\"\n",
    "SELECT\n",
    "\tb.id_unico,\t\n",
    "    b.nome_usuario,\n",
    "    b.data_rota,\n",
    "    b.inicio_rota,\n",
    "    b.fim_rota,\n",
    "    b.tempo_viagem,\n",
    "    b.first_lat,\n",
    "\tb.first_long,\n",
    "\tb.last_lat,\n",
    "\tb.last_long,\n",
    "    b.cidade,\n",
    "    b.estado,\n",
    "    b.pais,\n",
    "    b.dist_linear_km,\n",
    "    b.dist_manhattan_km,\t\n",
    "\tDATE(b.carga_banco)\n",
    "FROM (\n",
    "    select\n",
    "\t\tconcat(a.id_rota,'__',a.data_rota,'__',a.inicio_rota,'__',a.nome_usuario) as id_unico,        \n",
    "        a.nome_usuario,\n",
    "        a.data_rota,\n",
    "        a.inicio_rota,\n",
    "        a.fim_rota,\n",
    "\t\t\t\tcase when a.fim_rota > a.inicio_rota then a.fim_rota - a.inicio_rota\n",
    "        \t when a.inicio_rota > a.fim_rota then a.inicio_rota - a.fim_rota end as tempo_viagem,\n",
    "        a.first_lat,\n",
    "        a.first_long,\n",
    "        a.last_lat,\n",
    "        a.last_long,\n",
    "        a.cidade,\n",
    "        a.estado,\n",
    "        a.pais,\n",
    "        st_distance(\n",
    "            st_geographyfromtext(((('POINT('::text || a.last_long::text) || ' '::text) || a.last_lat::text) || ')'::text),\n",
    "            st_geographyfromtext(((('POINT('::text || a.first_long::text) || ' '::text) || a.first_lat::text) || ')'::text)\n",
    "        ) / 1000.0::double precision AS dist_linear_km,\n",
    "        abs(cast(a.last_long as numeric) - cast(a.first_long as NUMERIC)) + abs(cast(a.last_lat as NUMERIC)- cast(a.first_lat as NUMERIC)) * 1000.0::double precision AS dist_manhattan_km,\n",
    "        a.carga_banco\n",
    "    FROM (\n",
    "        SELECT DISTINCT\n",
    "            tb_gpx_full.id_rota,\n",
    "            tb_gpx_full.nome_usuario,\n",
    "            tb_gpx_full.data_rota, \n",
    "            tb_gpx_full.cidade,\n",
    "            tb_gpx_full.estado,\n",
    "            tb_gpx_full.pais,\n",
    "            first_value(tb_gpx_full.hora_rota) OVER (PARTITION BY tb_gpx_full.id_rota ORDER BY tb_gpx_full.id_rota)::time without time zone AS inicio_rota,\n",
    "            last_value(tb_gpx_full.hora_rota) OVER (PARTITION BY tb_gpx_full.id_rota ORDER BY tb_gpx_full.id_rota)::time without time zone AS fim_rota,\n",
    "            first_value(tb_gpx_full.longitude) OVER (PARTITION BY tb_gpx_full.id_rota ORDER BY tb_gpx_full.id_rota) AS first_long,\n",
    "            last_value(tb_gpx_full.longitude) OVER (PARTITION BY tb_gpx_full.id_rota ORDER BY tb_gpx_full.id_rota) AS last_long,\n",
    "            first_value(tb_gpx_full.latitude) OVER (PARTITION BY tb_gpx_full.id_rota ORDER BY tb_gpx_full.id_rota) AS first_lat,\n",
    "            last_value(tb_gpx_full.latitude) OVER (PARTITION BY tb_gpx_full.id_rota ORDER BY tb_gpx_full.id_rota) AS last_lat,\n",
    "            carga_banco::timestamp\n",
    "        FROM tb_gpx_full\n",
    "    ) a\n",
    ") b\n",
    "where \tb.data_rota is not null \n",
    "and \tb.inicio_rota is not null \n",
    "and \tb.tempo_viagem is not null\n",
    "and     DATE(b.carga_banco) = date(now())\n",
    "\n",
    "ORDER BY b.nome_usuario, b.data_rota, b.dist_manhattan_km desc ;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(consulta_veiculos)\n",
    "resultados_dist_veiculos = cursor.fetchall()\n",
    "conn.close()\n",
    "df = pd.DataFrame(resultados_dist_veiculos, columns=[desc[0] for desc in cursor.description])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_unico = df.loc[df.index[0], 'id_unico']\n",
    "# nome_usuario = df.loc[df.index[0], 'nome_usuario']\n",
    "\n",
    "#data_inicio_rota = datetime.strptime(df.loc[df.index[0], 'data'], '%Y-%m-%d').date()\n",
    "#data_fim_rota = datetime.strptime(df.loc[df.index[-1], 'data'], '%Y-%m-%d').date()\n",
    "#inicio_rota = datetime.strptime(df.loc[df.index[0], 'hora'], '%H:%M:%S').time()\n",
    "#inicio_rota = datetime.combine(datetime.min, inicio_rota)\n",
    "#fim_rota = datetime.strptime(df.loc[df.index[-1], 'hora'], '%H:%M:%S').time()\n",
    "#fim_rota = datetime.combine(datetime.min, fim_rota)\n",
    "#latitude_inicial = df.loc[df.index[0], 'latitude']\n",
    "#longitude_inicial = df.loc[df.index[0], 'longitude']\n",
    "#latitude_final = df.loc[df.index[-1], 'latitude']\n",
    "#longitude_final = df.loc[df.index[-1], 'longitude']\n",
    "#cidade = df.loc[df.index[0], 'cidade']\n",
    "#estado = df.loc[df.index[0], 'estado']\n",
    "#pais = df.loc[df.index[0], 'pais']\n",
    "#if data_inicio_rota == data_fim_rota:\n",
    "#    data_rota = data_inicio_rota\n",
    "#    if inicio_rota > fim_rota:\n",
    "#        tempo_viagem_real = inicio_rota - fim_rota\n",
    "#    elif fim_rota > inicio_rota:\n",
    "#        tempo_viagem_real = fim_rota - inicio_rota\n",
    "#    else:\n",
    "#        tempo_viagem_real = '0'\n",
    "#elif data_inicio_rota < data_fim_rota:\n",
    "#    data_rota = data_inicio_rota\n",
    "#    tempo_viagem_real = fim_rota - inicio_rota\n",
    "#    tempo_viagem_real = str(tempo_viagem_real)\n",
    "#    tempo_viagem_real = tempo_viagem_real.split(', ')\n",
    "#    tempo_viagem_real = tempo_viagem_real[1]\n",
    "#else:\n",
    "#    tempo_viagem_real = '0'\n",
    "#horas, minutos, segundos = map(int, tempo_viagem_real.split(':'))\n",
    "#tempo_viagem_formatado = f'{horas:02d}:{minutos:02d}:{segundos:02d}'\n",
    "\n",
    "#dist_linear = GD(lat_long_ini,lat_long_fin).km\n",
    "#\n",
    "\n",
    "#dist_manhattan_km = \n",
    "\n",
    "\n",
    "#lat_long_ini = float(first_lat),float(first_long)\n",
    "#lat_long_fin = float(last_lat),float(last_long)\n",
    "\n",
    "id_unicos = df['id_unico'].to_list()\n",
    "firsts_lat = df['first_lat'].to_list()\n",
    "firsts_long = df['first_long'].to_list()\n",
    "lasts_lat = df['last_lat'].to_list()\n",
    "lasts_long = df['last_long'].to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for first_long, first_lat, last_long, last_lat,id_unico in zip(firsts_long,firsts_lat,lasts_long,lasts_lat,id_unicos):        \n",
    "        coords = f'(({first_long},{first_lat}),({last_long},{last_lat}))'\n",
    "        coords = ast.literal_eval(coords)\n",
    "        client = openrouteservice.Client(key=token_key) # Specify your personal API key\n",
    "        response = client.directions(coords)\n",
    "        routes = response['routes']\n",
    "        summary = routes[0]['summary']\n",
    "        distancia_real = summary['distance']\n",
    "        distancia_real = distancia_real / 1000\n",
    "except Exception as e:\n",
    "    # Em caso de erro, imprime a mensagem de erro\n",
    "    print(f\"Erro: {str(e)} no id {id_unico}\")\n",
    "    continue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = f'(({first_long},{first_lat}),({last_long},{last_lat}))'\n",
    "coords = ast.literal_eval(coords)\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openrouteservice.Client(key=token_key) # Specify your personal API key\n",
    "response = client.directions(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = response['routes']\n",
    "summary = routes[0]['summary']\n",
    "distancia_real = summary['distance']\n",
    "distancia_real = distancia_real / 1000\n",
    "tempo_viagem_estimado = summary['duration']\n",
    "horas, segundos_restantes = divmod(tempo_viagem_estimado, 3600)\n",
    "minutos, segundos = divmod(segundos_restantes, 60)\n",
    "tempo_viagem_estimado_formatado = '{:02}:{:02}:{:02}'.format(int(horas), int(minutos), int(segundos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_viagem_estimado_formatado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "insert = f'''\n",
    "insert into tb_distancias_percorridas (id_unico,nome_usuario,data_rota,inicio_rota,fim_rota,tempo_viagem_real,tempo_viagem_estimado ,cidade,estado,pais,dist_real)\n",
    "values (\n",
    "        '{id_unico}',\n",
    "        '{nome_usuario}',\n",
    "        '{data_rota}',\n",
    "        '{inicio_rota.strftime('%H:%M:%S')}',\n",
    "        '{fim_rota.strftime('%H:%M:%S')}',\n",
    "        '{tempo_viagem_formatado}',\n",
    "        '{tempo_viagem_estimado_formatado}',\n",
    "        '{cidade}',\n",
    "        '{estado}',\n",
    "        '{pais}',\n",
    "        '{distancia_real}'\n",
    "        )\n",
    "'''\n",
    "print(insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(insert)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "Latitude = \"45.24559097710776\"\n",
    "Longitude = \"-122.79781984274484\"\n",
    "location = geolocator.reverse(f'{Latitude},{Longitude}') \n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = location.raw['address']\n",
    "address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = location.raw['address']\n",
    "city = address.get('city', '')\n",
    "state = address.get('state', '')\n",
    "country = address.get('country', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "Latitude = \"45.24559097710776\"\n",
    "Longitude = \"-122.79781984274484\"\n",
    "location = geolocator.reverse(f'{Latitude},{Longitude}') \n",
    "print(location)\n",
    "\n",
    "address = location.raw['address']\n",
    "cidade = address.get('suburb')\n",
    "estado = address.get('ISO3166-2-lvl4')\n",
    "pais = address.get('country_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cidade:\", cidade)\n",
    "print(\"Estado (abreviado):\", estado)\n",
    "print(\"País:\", pais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'gold-saint',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"SELECT * FROM tb_gpx_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**db_config)\n",
    "\n",
    "df_pandas = pd.read_sql_query(sql_query, conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df_pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_info_localizacao(df):\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    latitudes = df['latitude'].astype(str)\n",
    "    longitudes = df['longitude'].astype(str)\n",
    "\n",
    "    def obter_localizacao(latitude, longitude):\n",
    "        location = geolocator.reverse(f'{latitude},{longitude}')\n",
    "        address = location.raw['address']\n",
    "        cidade = address.get('suburb')\n",
    "        estado = address.get('state')\n",
    "        pais = address.get('country_code')\n",
    "        return cidade, estado, pais\n",
    "    df[['cidade', 'estado', 'pais']] = zip(*[obter_localizacao(lat, lon) for lat, lon in zip(latitudes, longitudes)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultante = obter_info_localizacao(df_pandas)\n",
    "df_resultante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping para obter o arquivo com o preco médio de combustível -- Pendente finalizar o webscrapping da tabela da ANP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das bibliotecas necessárias para a raspagem de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "# pip install webdriver-manager\n",
    "# pip install BeautifulSoup4\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "servico = Service(ChromeDriverManager().install())\n",
    "import os\n",
    "import pandas as pd\n",
    "from minio import Minio #--> O módulo minio é usado para interagir com um servidor MinIO\n",
    "from minio.error import S3Error #--> O módulo S3Error é uma exceção específica do MinIO para exibição de forma semelhante ao Amazon S3\n",
    "from io import StringIO, BytesIO\n",
    "import psycopg2\n",
    "import io\n",
    "\n",
    "\n",
    "db_config = {\n",
    "'host': 'localhost',\n",
    "'database': 'postgres',\n",
    "'user': 'postgres',\n",
    "'password': 'postgres',\n",
    "}\n",
    "\n",
    "\n",
    "URL_ANP_PRECO_MEDIO_SEMANAL = 'https://www.gov.br/anp/pt-br/assuntos/precos-e-defesa-da-concorrencia/precos/levantamento-de-precos-de-combustiveis-ultimas-semanas-pesquisadas' #--> Página do OpenstreetMap onde estão localizadas as rotas para download.\n",
    "DOWNLOADS = '/home/thiago/Downloads/'\n",
    "DADOS_COMBUSTIVEL = '/home/thiago/tcc_ufrj/DADOS_COMBUSTIVEL/'\n",
    "BUCKET_DADOS_COMBUSTIVEL = 'dados-combustivel'\n",
    "\n",
    "\n",
    "\n",
    "minioclient = Minio('localhost:9000', #--> O cliente é configurado para se conectar a um servidor MinIO local usando as credenciais fornecidas\n",
    "    access_key='minioadmin', #--> A chave de acesso = usuário\n",
    "    secret_key='minioadmin', #--> A chave secreta = Senha \n",
    "    secure=False) #--> Sem usar conexão segura (HTTPS).\n",
    "\n",
    "\n",
    "# navegador = webdriver.Chrome(service=servico, options=options) #--> Aplicando as opções acima mencionadas no navegador.\n",
    "\n",
    "# navegador = webdriver.Chrome(service=servico)\n",
    "# navegador.get(URL_ANP_PRECO_MEDIO_SEMANAL)\n",
    "\n",
    "# conteudo_da_pagina = navegador.page_source\n",
    "# site = BeautifulSoup(conteudo_da_pagina, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformando o xlsx em um Pandas DataFrame \n",
    "arquivos_combustivel = [arquivo for arquivo in os.listdir(DADOS_COMBUSTIVEL) if arquivo.endswith(\".xlsx\")] #--> Listando todos os arquivos da pasta pré-processamento com extensão .gpx\n",
    "\n",
    "for arquivo_combustivel in arquivos_combustivel:\n",
    "    caminho_arquivo = os.path.join(DADOS_COMBUSTIVEL, arquivo_combustivel) #--> Criando o caminho completo para o arquivo .gpx        \n",
    "    df = pd.read_excel(caminho_arquivo, sheet_name='CAPITAIS', skiprows=9)\n",
    "    df.to_csv(f'{DADOS_COMBUSTIVEL}PRECO_NACIONAL.csv', sep=';', encoding='utf8', index=False)\n",
    "    df.head()\n",
    "# df.dtypes\n",
    "# df_rj = df[df['MUNICÍPIO'] == 'RIO DE JANEIRO']\n",
    "# df_rj.to_csv(f'{DADOS_COMBUSTIVEL}PRECO_RJ.csv', sep=';', encoding='utf8', index=False)\n",
    "# df_rj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enviando CSV para o Datalake\n",
    "arquivos_para_datalake = [arquivo for arquivo in os.listdir(DADOS_COMBUSTIVEL) if arquivo.endswith(\".csv\")] #--> Listando todos os arquivos da pasta pré-processamento com extensão .csv\n",
    "for nome_arquivo in arquivos_para_datalake: #--> Iterando sobre cada item da lista\n",
    "    caminho_pre_proc = os.path.join(DADOS_COMBUSTIVEL, nome_arquivo) #--> Criando o caminho completo para o arquivo .csv    \n",
    "    if os.path.isfile(caminho_pre_proc): #--> Verificando se o caminho especificado está apontando para um arquivo válido no sistema de arquivos.\n",
    "        try:\n",
    "            minioclient.fput_object(BUCKET_DADOS_COMBUSTIVEL, nome_arquivo, caminho_pre_proc) #--> Usando o cliente Minio para enviar o arquivo da pasta de pré processamento para o bucket especificado (CAMADA_BRONZE)\n",
    "            #print(f\"Arquivo {nome_arquivo} enviado com sucesso para o bucket.\") #--> Exibindo a mensagem de sucesso após o upload dos arquivos para o bucket.\n",
    "            os.remove(caminho_pre_proc) # --> Após o envio bem sucedido para o bucket o arquivo é excluído da pasta DADOS_COMBUSTIVEL\n",
    "        except S3Error as e: #--> Capturando qualquer erro que porventura ocorra\n",
    "            print(f\"Erro ao enviar o arquivo: {nome_arquivo} -> Erro: {e}\") #--> Exibindo o erro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conexão e escrita no banco de dados do arquivo CSV obtido do Bucket\n",
    "try:\n",
    "    # Lista todos os arquivos na camada \"gold\" do Minio que têm extensão .csv\n",
    "    arquivos_preco_combustivel = [arquivo_gpx for arquivo_gpx in minioclient.list_objects(BUCKET_DADOS_COMBUSTIVEL) if arquivo_gpx.object_name.endswith(\".csv\")]\n",
    "    \n",
    "    # Verifica se há arquivos no bucket antes de continuar\n",
    "    if not arquivos_preco_combustivel:\n",
    "        print(\"Não existem arquivos CSV no bucket. Nenhuma carga de dados será executada.\")\n",
    "\n",
    "    else:\n",
    "        # Conexão com o banco de dados PostgreSQL\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        #truncate = \"\"\" truncate table preco_combustivel_semanal; \"\"\"\n",
    "        #cursor.execute(truncate)\n",
    "\n",
    "        copy_sql = \"\"\"\n",
    "            COPY preco_combustivel_semanal (data_inicial,data_final,estado,municipio,produto,numero_de_postos_pesquisados,unidade_de_medida,preco_medio_revenda,desvio_padrao_revenda,preco_minimo_revenda,preco_maximo_revenda,coef_de_variacao_revenda)\n",
    "            FROM stdin WITH CSV HEADER DELIMITER as ';'\n",
    "        \"\"\"\n",
    "\n",
    "        # Itera sobre cada arquivo CSV encontrado no Minio\n",
    "        for arquivo_preco_combustivel in arquivos_preco_combustivel:\n",
    "            # Obtém o objeto do arquivo CSV do Minio  \n",
    "            obj_preco = minioclient.get_object(BUCKET_DADOS_COMBUSTIVEL, arquivo_preco_combustivel.object_name)            \n",
    "\n",
    "            # Decodifica os dados do arquivo CSV de bytes para string\n",
    "            csv_decod = obj_preco.data.decode('utf-8')  # Convertendo bytes para string\n",
    "            arquivo_csv = StringIO(csv_decod)\n",
    "            df = pd.read_csv(arquivo_csv, sep=';')\n",
    "            csv_bytes = df.to_csv(index=False,sep=';').encode('utf-8')\n",
    "            csv_buffer = BytesIO(csv_bytes)\n",
    "            nome_arquivo = arquivo_preco_combustivel.object_name\n",
    "\n",
    "\n",
    "            # Usa io.StringIO para criar um objeto de arquivo legível a partir da string CSV\n",
    "            with io.StringIO(csv_decod) as file:        \n",
    "\n",
    "                # Executa o comando COPY para inserir os dados no banco de dados PostgreSQL\n",
    "                cursor.copy_expert(sql=copy_sql, file=file)\n",
    "\n",
    "            ## Commit para salvar as alterações no banco de dados    \n",
    "            conn.commit()\n",
    "\n",
    "            # Após a copia para o bucket de segurança os arquivos são eliminados da camada gold\n",
    "            minioclient.remove_object(BUCKET_DADOS_COMBUSTIVEL, arquivo_preco_combustivel.object_name) \n",
    "\n",
    "        # Fecha a conexão com o banco de dados PostgreSQL\n",
    "        conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    # Em caso de erro, imprime a mensagem de erro\n",
    "    print(f\"Erro: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API para criar tabela com preco de combustível -- API para cirar um DF com o consumo de cada veículo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para o combustivel - fazer um webscraping da tabela da anp\n",
    "# https://www.gov.br/anp/pt-br/assuntos/precos-e-defesa-da-concorrencia/precos/precos-de-distribuicao-de-combustiveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo novamente a conexão com o banco para selecionar da tabela gpx full somente o nome a marca e o modelo dos veículos\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "import requests\n",
    "\n",
    "consulta_usuarios_gpx_full = \"\"\"\n",
    "select distinct\n",
    "split_part(LOWER(nome_usuario),'_',1) as marca,\n",
    "split_part(LOWER(nome_usuario),'_',2) as modelo\n",
    "from tb_gpx_full\n",
    "\n",
    "\"\"\"\n",
    "cursor.execute(consulta_usuarios_gpx_full)\n",
    "resultados_usuarios_gpx_full = cursor.fetchall()\n",
    "conn.close()\n",
    "df_usuarios_full = pd.DataFrame(resultados_usuarios_gpx_full, columns=[desc[0] for desc in cursor.description])\n",
    "df_usuarios_full.head()\n",
    " \n",
    "df_marcas = df_usuarios_full['marca'].tolist()\n",
    "df_modelos = df_usuarios_full['modelo'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_marcas \n",
    "#df_modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marca = 'ram'\n",
    "df_modelo = '1500'\n",
    "df_carros = pd.DataFrame()\n",
    "api_url = 'https://api.api-ninjas.com/v1/cars'\n",
    "api_key = '/DHg+PPb3h7gYITeEup54w==KXt6OHpmw3zMNgfE'\n",
    "params = {'make': df_marca, 'model': df_modelo}\n",
    "headers = {'X-Api-Key': api_key}\n",
    "response = requests.get(api_url, params=params, headers=headers)\n",
    "        \n",
    "if response.status_code == requests.codes.ok:            \n",
    "    data = response.json()\n",
    "    df_car = pd.DataFrame(data)\n",
    "    df_carros = pd.concat([df_carros, df_car], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumindo a API para criar um DF com o consumo em litros por KM dos veículos\n",
    "df_carros = pd.DataFrame()\n",
    "api_url = 'https://api.api-ninjas.com/v1/cars'\n",
    "api_key = '/DHg+PPb3h7gYITeEup54w==KXt6OHpmw3zMNgfE'\n",
    "\n",
    "for df_marca, df_modelo in  zip(df_marcas, df_modelos):\n",
    "    params = {'make': df_marca, 'model': df_modelo}\n",
    "    headers = {'X-Api-Key': api_key}\n",
    "    response = requests.get(api_url, params=params, headers=headers)\n",
    "        \n",
    "    if response.status_code == requests.codes.ok:            \n",
    "        data = response.json()\n",
    "        df_car = pd.DataFrame(data)\n",
    "        df_carros = pd.concat([df_carros, df_car], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carros.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carros['city_km/l'] = (df_carros['city_mpg'] * 1.609344) / 3.785411784\n",
    "df_carros['highway_km/l'] = (df_carros['highway_mpg'] * 1.609344) / 3.785411784\n",
    "df_carros.drop('city_mpg', axis=1, inplace=True)\n",
    "df_carros.drop('highway_mpg', axis=1, inplace=True)\n",
    "df_carros.drop('combination_mpg', axis=1, inplace=True)\n",
    "\n",
    "df_carros.to_csv(f'{DADOS_COMBUSTIVEL}consumo_veiculos.csv', sep=';', encoding='utf8', index=False)\n",
    "#print(df_carros.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enviando CSV para o Datalake\n",
    "arquivos_para_datalake = [arquivo for arquivo in os.listdir(DADOS_COMBUSTIVEL) if arquivo.endswith(\".csv\")] #--> Listando todos os arquivos da pasta pré-processamento com extensão .csv\n",
    "arquivos_para_datalake\n",
    "for nome_arquivo in arquivos_para_datalake: #--> Iterando sobre cada item da lista\n",
    "    caminho_pre_proc = os.path.join(DADOS_COMBUSTIVEL, nome_arquivo) #--> Criando o caminho completo para o arquivo .csv    \n",
    "    if os.path.isfile(caminho_pre_proc): #--> Verificando se o caminho especificado está apontando para um arquivo válido no sistema de arquivos.\n",
    "        try:\n",
    "            minioclient.fput_object(BUCKET_DADOS_COMBUSTIVEL, nome_arquivo, caminho_pre_proc) #--> Usando o cliente Minio para enviar o arquivo da pasta de pré processamento para o bucket especificado (CAMADA_BRONZE)\n",
    "            #print(f\"Arquivo {nome_arquivo} enviado com sucesso para o bucket.\") #--> Exibindo a mensagem de sucesso após o upload dos arquivos para o bucket.\n",
    "            os.remove(caminho_pre_proc) # --> Após o envio bem sucedido para o bucket o arquivo é excluído da pasta DADOS_COMBUSTIVEL\n",
    "        except S3Error as e: #--> Capturando qualquer erro que porventura ocorra\n",
    "            print(f\"Erro ao enviar o arquivo: {nome_arquivo} -> Erro: {e}\") #--> Exibindo o erro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conexão e escrita no banco de dados do arquivo CSV obtido do Bucket\n",
    "try:\n",
    "    # Lista todos os arquivos na camada \"gold\" do Minio que têm extensão .csv\n",
    "    arquivo_consumo_veiculos = [arquivo for arquivo in minioclient.list_objects(BUCKET_DADOS_COMBUSTIVEL) if arquivo.object_name.endswith(\".csv\")]\n",
    "    \n",
    "    # Verifica se há arquivos no bucket antes de continuar\n",
    "    if not arquivo_consumo_veiculos:\n",
    "        print(\"Não existem arquivos CSV no bucket. Nenhuma carga de dados será executada.\")\n",
    "\n",
    "    else:\n",
    "        # Conexão com o banco de dados PostgreSQL\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        truncate = \"\"\" truncate table tb_consumo_veiculos; \"\"\"\n",
    "        cursor.execute(truncate)\n",
    "\n",
    "        copy_sql = \"\"\"\n",
    "            COPY tb_consumo_veiculos (classe,drive,fuel_type,make,model,transmission,years,cylinders,displacement,city_km_l,highway_km_l)\n",
    "            FROM stdin WITH CSV HEADER DELIMITER as ';'\n",
    "        \"\"\"\n",
    "\n",
    "        # Itera sobre cada arquivo CSV encontrado no Minio\n",
    "        for arquivo_consumo_veiculo in arquivo_consumo_veiculos:\n",
    "            # Obtém o objeto do arquivo CSV do Minio  \n",
    "            obj_preco = minioclient.get_object(BUCKET_DADOS_COMBUSTIVEL, arquivo_consumo_veiculo.object_name)            \n",
    "\n",
    "            # Decodifica os dados do arquivo CSV de bytes para string\n",
    "            csv_decod = obj_preco.data.decode('utf-8')  # Convertendo bytes para string\n",
    "            arquivo_csv = StringIO(csv_decod)\n",
    "            df = pd.read_csv(arquivo_csv, sep=';')\n",
    "            csv_bytes = df.to_csv(index=False,sep=';').encode('utf-8')\n",
    "            csv_buffer = BytesIO(csv_bytes)\n",
    "            nome_arquivo = arquivo_consumo_veiculo.object_name\n",
    "\n",
    "\n",
    "            # Usa io.StringIO para criar um objeto de arquivo legível a partir da string CSV\n",
    "            with io.StringIO(csv_decod) as file:        \n",
    "\n",
    "                # Executa o comando COPY para inserir os dados no banco de dados PostgreSQL\n",
    "                cursor.copy_expert(sql=copy_sql, file=file)\n",
    "\n",
    "            ## Commit para salvar as alterações no banco de dados    \n",
    "            conn.commit()\n",
    "\n",
    "            # Após a copia para o bucket de segurança os arquivos são eliminados da camada gold\n",
    "            minioclient.remove_object(BUCKET_DADOS_COMBUSTIVEL, arquivo_consumo_veiculo.object_name) \n",
    "\n",
    "        # Fecha a conexão com o banco de dados PostgreSQL\n",
    "        conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    # Em caso de erro, imprime a mensagem de erro\n",
    "    print(f\"Erro: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "make = df_marca\n",
    "model = df_modelo\n",
    "api_url = 'https://api.api-ninjas.com/v1/cars?model={}'.format(make,model)\n",
    "api_key = '/DHg+PPb3h7gYITeEup54w==KXt6OHpmw3zMNgfE'\n",
    "response = requests.get(api_url, headers={'X-Api-Key': api_key})\n",
    "if response.status_code == requests.codes.ok:    \n",
    "    #df = pd.DataFrame(response.text)\n",
    "    data = response.json()\n",
    "    df_car = pd.DataFrame(data)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.api-ninjas.com/v1/cars?model={}'.format('skoda','kodiaq')\n",
    "response = requests.get(api_url, headers={'X-Api-Key': '/DHg+PPb3h7gYITeEup54w==KXt6OHpmw3zMNgfE'})\n",
    "if response.status_code == requests.codes.ok:       \n",
    "    data = response.json()\n",
    "    df_car = pd.DataFrame(data)\n",
    "\n",
    "df_car   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "make = df_marca  # Substitua df_marca pelo valor desejado\n",
    "model = df_modelo  # Substitua df_modelo pelo valor desejado\n",
    "api_url = 'https://api.api-ninjas.com/v1/cars?model={}'.format(make,model)\n",
    "headers = {'X-Api-Key': '/DHg+PPb3h7gYITeEup54w==KXt6OHpmw3zMNgfE'}\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Converte o JSON da resposta em um objeto Python\n",
    "        df = pd.DataFrame(data)  # Cria um DataFrame com base nos dados\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"Erro:\", response.status_code, response.text)\n",
    "except Exception as e:\n",
    "    print(\"Erro ao fazer a solicitação à API:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_carros['km_cidade'] = df_carros['city_mpg'] * 1.60934\n",
    "#df_carros['km_estrada'] = df_carros['highway_mpg'] * 1.60934\n",
    "#df_carros['litro_cidade'] = df_carros['city_mpg'] * 3.78541\n",
    "#df_carros['litro_estrada'] = df_carros['highway_mpg'] * 3.78541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convertendo milha para Km - (1 milha equivale a aproximadamente 1,60934 km)\n",
    "km_cidade = df_carros['city_mpg'] * 1.60934\n",
    "km_estrada = df_carros['highway_mpg'] * 1.60934\n",
    "\n",
    "## Convertendo galao para litro - (1 galão equivale a aproximadamente 3,78541 litros)\n",
    "litro_cidade = df_carros['city_mpg'] * 3.78541\n",
    "litro_estrada = df_carros['highway_mpg'] * 3.78541\n",
    "\n",
    "km_cidade =  km_cidade.mean()\n",
    "km_estrada =  km_estrada.mean()\n",
    "litro_cidade = litro_cidade.mean()\n",
    "litro_estrada = litro_estrada.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = litro_cidade / km_cidade\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando o consumo em litros por quilômetro (L/km)\n",
    "df['consumo_litros_por_km_cidade'] = litro_cidade / km_cidade\n",
    "df['consumo_litros_por_km_estrada'] = litro_estrada / km_estrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando o consumo em km/L\n",
    "df['consumo_km_por_litro_cidade'] = km_cidade / litro_cidade\n",
    "df['consumo_km_por_litro_estrada'] = km_estrada / litro_estrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio da Utilização de modelos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from minio import Minio\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "data_hora_atual = datetime.now()\n",
    "REGRESSAO_LINEAR = 'regressao-linear'\n",
    "ML_RESULTS = 'resultados-ml'\n",
    "\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)\n",
    "\n",
    "db_config = {\n",
    "'host': 'localhost',\n",
    "'database': 'postgres',\n",
    "'user': 'postgres',\n",
    "'password': 'postgres',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo conexão com o banco para selecionar a view \"vw_valores_combustivel_por_viagem\" com os dados que serão utilizados para a regressão linear.\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "consulta_veiculos = \"\"\"\n",
    "select \n",
    "\tdata_rota,\t\n",
    "\tnome_usuario,\n",
    "\ttipo_combustivel,\t\n",
    "\tdist_km,\n",
    "\tcons_por_km as consumo_litros_viagem\t\n",
    "from vw_valores_combustivel_por_viagem \n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(consulta_veiculos)\n",
    "resultados_consumo_veiculos = cursor.fetchall()\n",
    "conn.close()\n",
    "df_consumo_veiculos = pd.DataFrame(resultados_consumo_veiculos, columns=[desc[0] for desc in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumo_veiculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo variáveis independentes (X) e dependente (Y)\n",
    "X = df_consumo_veiculos[['dist_km','nome_usuario']]\n",
    "Y = df_consumo_veiculos['consumo_litros_viagem']\n",
    "X = pd.get_dummies(X, columns=['nome_usuario'], drop_first=True)\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Inicializando o modelo de regressão linear\n",
    "model = LinearRegression()\n",
    "\n",
    "# Treinando o modelo\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Fazendo previsões no conjunto de teste\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "print('Erro Médio Absoluto:', metrics.mean_absolute_error(Y_test, Y_pred))\n",
    "print('Erro Quadrático Médio:', metrics.mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "\n",
    "coeficientes = model.coef_\n",
    "coeficientes_df = pd.DataFrame({'Variável': X.columns, 'Coeficiente': coeficientes})\n",
    "coeficientes_df['Coeficiente_Abs'] = coeficientes_df['Coeficiente'].abs().apply(lambda x: format(x, '.6f'))\n",
    "coeficientes_df = coeficientes_df.sort_values(by='Coeficiente_Abs', ascending=False)\n",
    "\n",
    "#valor_formatado = format(5.551115e-17, '.10f')\n",
    "#valor_formatado\n",
    "#valor_arredondado = round(5.551115e-17, 10)\n",
    "#valor_arredondado\n",
    "\n",
    "# Converta o DataFrame enriquecido de volta para CSV\n",
    "csv_to_regressao_linear = coeficientes_df.to_csv(index=False, sep=';')\n",
    "csv_to_regressao_linear_bytes = csv_to_regressao_linear.encode('utf-8')\n",
    "\n",
    "# Crie um buffer de bytes\n",
    "csv_to_regressao_linear_buffer = BytesIO(csv_to_regressao_linear_bytes)\n",
    "nome_arquivo = f'regressao_linear_{data_hora_atual}.csv'\n",
    "\n",
    "minioclient.put_object(\n",
    "        REGRESSAO_LINEAR,\n",
    "        nome_arquivo,  # Use o mesmo nome de arquivo\n",
    "        data=csv_to_regressao_linear_buffer,\n",
    "        length=len(csv_to_regressao_linear_bytes),\n",
    "        content_type='application/csv')\n",
    "\n",
    "## Enviando a regressão Linear para o Banco \n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "truncate = \"\"\" truncate table regressao_linear_temp; \"\"\"\n",
    "cursor.execute(truncate)\n",
    "\n",
    "copy_sql = \"\"\"\n",
    "    COPY regressao_linear_temp (variavel, coeficiente, coeficiente_abs)\n",
    "    FROM stdin WITH CSV HEADER DELIMITER as ';'\"\"\"\n",
    "\n",
    "\n",
    "arquivos_regressao = [arquivo for arquivo in minioclient.list_objects(REGRESSAO_LINEAR) if arquivo.object_name.endswith(\".csv\")]\n",
    "# Itera sobre cada arquivo CSV encontrado no Minio\n",
    "for arquivo_regressao in arquivos_regressao:\n",
    "    # Obtém o objeto do arquivo CSV do Minio  \n",
    "    obj_regr = minioclient.get_object(REGRESSAO_LINEAR, arquivo_regressao.object_name)\n",
    "    csv_decod = obj_regr.data.decode('utf-8')  # Convertendo bytes para string\n",
    "    arquivo_csv = StringIO(csv_decod)\n",
    "    df = pd.read_csv(arquivo_csv, sep=';')\n",
    "    csv_bytes = df.to_csv(index=False,sep=';').encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "    nome_arquivo = arquivo_regressao.object_name\n",
    "\n",
    "    with io.StringIO(csv_decod) as file:\n",
    "        cursor.copy_expert(sql=copy_sql, file=file)\n",
    "    conn.commit()\n",
    "\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "insert = \"\"\"\n",
    "insert into regressao_linear (variavel, coeficiente, coeficiente_abs, data_execucao )\n",
    "select \treplace (variavel,'nome_usuario_','') as variavel, \n",
    "\t\tCoeficiente, \n",
    "\t\tCoeficiente_Abs, \n",
    "\t\tnow() as data_execucao \n",
    "from regressao_linear_temp\n",
    "    \"\"\"\n",
    "cursor.execute(insert)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "minioclient.put_object(\n",
    "        ML_RESULTS,\n",
    "        arquivo_regressao.object_name,  # Use o mesmo nome de arquivo\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv')\n",
    "\n",
    "minioclient.remove_object(REGRESSAO_LINEAR, nome_arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Com essa Regressão feita temos acesso a um indicativo do veículo que mais consome combustível \n",
    "#### Pode-se considerar que as informações acima do dist_km são influenciadas pelo nome_usuario e abaixo do dist_km são influenciadas pela distancia\n",
    "\n",
    "#### Entre os tipos de veículos, o RAM_1500_5TH_GEN tem o maior impacto absoluto no consumo de combustível, com um coeficiente absoluto de 0.507708.\n",
    "\n",
    "#### Quando consideramos a distância (dist_km), o veículo do tipo HYUNDAI_SANTA_FE_HYBRID_2022 tem o maior impacto absoluto no consumo de combustível entre os veículos listados, com um coeficiente absoluto de 0.092526.\n",
    "\n",
    "#### Essas interpretações são feitas com base nos coeficientes do modelo de regressão linear que você treinou. Vale ressaltar que o impacto é relativo às outras categorias ou variáveis incluídas no modelo. Portanto, para a variável específica \"nome_usuario_HYUNDAI_SANTA_FE_HYBRID_2022\", o coeficiente absoluto indica a magnitude do impacto dessa categoria em relação à categoria de referência (ou base) para essa variável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamento (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versão Funcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from minio import Minio\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "data_hora_atual = datetime.now()\n",
    "from time import sleep\n",
    "from decimal import Decimal\n",
    "CLUSTERING = 'clustering'\n",
    "ML_RESULTS = 'resultados-ml'\n",
    "\n",
    "\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)\n",
    "\n",
    "\n",
    "db_config = {\n",
    "'host': 'localhost',\n",
    "'database': 'postgres',\n",
    "'user': 'postgres',\n",
    "'password': 'postgres',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "select\t\n",
    "\tnome_usuario,\n",
    "\tcase when tipo_combustivel = 'gas' then 1 end as tipo_combustivel, \t\n",
    "\tdist_km,\n",
    "\tmedia_consumo_km_l, \n",
    "\tcase when custo_trajeto = null then avg(custo_trajeto::numeric) else custo_trajeto end as custo_trajeto  \t\n",
    "from vw_valores_combustivel_por_viagem\n",
    "where tipo_combustivel = 'gas'\n",
    "group by\n",
    "\tnome_usuario,\n",
    "\ttipo_combustivel, \t\n",
    "\tdist_km,\n",
    "\tmedia_consumo_km_l,\n",
    "\tcusto_trajeto \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql_query)\n",
    "resultados_agrup_veiculos = cursor.fetchall()\n",
    "conn.close()\n",
    "df_agrup_veiculos = pd.DataFrame(resultados_agrup_veiculos, columns=[desc[0] for desc in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrup_veiculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando colunas relevantes\n",
    "X = df_agrup_veiculos[['dist_km', 'media_consumo_km_l']]\n",
    "\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Especificando o número de clusters\n",
    "num_clusters = 3  \n",
    "\n",
    "\n",
    "# Aplicando o K-means\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "df_agrup_veiculos['cluster_consumo'] = kmeans.fit_predict(X_normalized)\n",
    "\n",
    "\n",
    "# Criando um novo DataFrame com os resultados do agrupamento\n",
    "df_veiculos_clusterizados = df_agrup_veiculos[['nome_usuario', 'dist_km', 'media_consumo_km_l', 'cluster_consumo']].copy()\n",
    "\n",
    "\n",
    "# Adicionando informações adicionais conforme necessário\n",
    "# df_resultado['informacao_adicional'] = ...\n",
    "\n",
    "# Salvando o DataFrame resultante em um arquivo CSV ou inserindo no banco de dados\n",
    "df_veiculos_clusterizados.to_csv('resultado_agrupamento.csv', index=False)\n",
    "\n",
    "\n",
    "##--------------------------------------------------------------------------------------------------##\n",
    "##--------------------------------------------------------------------------------------------------##\n",
    "\n",
    "## Calculando a média do consumo de combustível para cada cluster\n",
    "#media_consumo_por_cluster = df_agrup_veiculos.groupby('cluster_consumo')['media_consumo_km_l'].mean()\n",
    "## Exibindo os resultados\n",
    "#for cluster, media_consumo in media_consumo_por_cluster.items():\n",
    "#    print(f\"\\nCluster {cluster} - Média de Consumo: {media_consumo}\")\n",
    "#    veiculos_cluster = df_agrup_veiculos[df_agrup_veiculos['cluster_consumo'] == cluster]\n",
    "#    print(veiculos_cluster[['nome_usuario', 'media_consumo_km_l']])\n",
    "\n",
    "##--------------------------------------------------------------------------------------------------##\n",
    "##--------------------------------------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "## Converta o DataFrame enriquecido de volta para CSV\n",
    "csv_veiculos_clusterizados = df_veiculos_clusterizados.to_csv(index=False, sep=';')\n",
    "csv_veiculos_clusterizados_bytes = csv_veiculos_clusterizados.encode('utf-8')\n",
    "\n",
    "## Crie um buffer de bytes\n",
    "csv_veiculos_clusterizados_buffer = BytesIO(csv_veiculos_clusterizados_bytes)\n",
    "nome_arquivo = f'clustering_{data_hora_atual}.csv'\n",
    "\n",
    "minioclient.put_object(\n",
    "        CLUSTERING,\n",
    "        nome_arquivo,  # Use o mesmo nome de arquivo\n",
    "        data=csv_veiculos_clusterizados_buffer,\n",
    "        length=len(csv_veiculos_clusterizados_bytes),\n",
    "        content_type='application/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enviando a regressão Linear para o Banco \n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "truncate = \"\"\" truncate table veiculos_clusterizados_temp; \"\"\"\n",
    "cursor.execute(truncate)\n",
    "\n",
    "copy_sql = \"\"\"\n",
    "    COPY veiculos_clusterizados_temp (nome_usuario, dist_km, media_consumo_km_l, cluster_consumo)\n",
    "    FROM stdin WITH CSV HEADER DELIMITER as ';'\"\"\"\n",
    "\n",
    "arquivos_clustering = [arquivo for arquivo in minioclient.list_objects(CLUSTERING) if arquivo.object_name.endswith(\".csv\")]\n",
    "## Itera sobre cada arquivo CSV encontrado no Minio\n",
    "for arquivo_clustering in arquivos_clustering:\n",
    "    # Obtém o objeto do arquivo CSV do Minio  \n",
    "    obj_regr = minioclient.get_object(CLUSTERING, arquivo_clustering.object_name)\n",
    "    csv_decod = obj_regr.data.decode('utf-8')  # Convertendo bytes para string\n",
    "    arquivo_csv = StringIO(csv_decod)\n",
    "    df = pd.read_csv(arquivo_csv, sep=';')\n",
    "    csv_bytes = df.to_csv(index=False,sep=';').encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "    nome_arquivo = arquivo_clustering.object_name\n",
    "\n",
    "    with io.StringIO(csv_decod) as file:\n",
    "        cursor.copy_expert(sql=copy_sql, file=file)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "insert = \"\"\"\n",
    "        insert into veiculos_clusterizados (nome_usuario, dist_km, media_consumo_km_l, cluster_consumo, data_execucao )\n",
    "        select \t\n",
    "        \tnome_usuario,\n",
    "        \tdist_km,\n",
    "        \tmedia_consumo_km_l,\n",
    "        \tcluster_consumo,\n",
    "        \tnow() as data_execucao \n",
    "        from veiculos_clusterizados_temp\n",
    "    \"\"\"\n",
    "cursor.execute(truncate)\n",
    "cursor.execute(insert)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "minioclient.put_object(\n",
    "        ML_RESULTS,\n",
    "        arquivo_clustering.object_name,  # Use o mesmo nome de arquivo\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv')\n",
    "\n",
    "\n",
    "minioclient.remove_object(CLUSTERING, arquivo_clustering.object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "pior_desempenho = \"\"\"\n",
    "select \n",
    "\tnome_usuario,\n",
    "\tround(media_consumo_km_l::numeric,2) as media_consumo_km_l,\n",
    "\tround(sum(dist_km::numeric),2) as dist_km, \n",
    "\tround(sum((dist_km::numeric / media_consumo_km_l::numeric) * (select AVG(preco_medio_revenda::NUMERIC) from preco_combustivel_semanal where estado = 'RIO DE JANEIRO' and produto like '%GAS%')),2) as custo_viagem \n",
    "from veiculos_clusterizados\n",
    "where cluster_consumo between '0' and '1'\n",
    "group by nome_usuario,\n",
    "\tmedia_consumo_km_l\n",
    "order by media_consumo_km_l::numeric asc\n",
    "limit 5\n",
    "\"\"\"\n",
    "cursor.execute(pior_desempenho)\n",
    "resultados_pior_desempenho = cursor.fetchall()\n",
    "conn.close()\n",
    "df_pior_desempenho = pd.DataFrame(resultados_pior_desempenho, columns=[desc[0] for desc in cursor.description])\n",
    "df_pior_desempenho = df_pior_desempenho.sort_values(by='custo_viagem')\n",
    "\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "melhor_desempenho = \"\"\"\n",
    "select \n",
    "\tnome_usuario,\n",
    "\tround(media_consumo_km_l::numeric, 2) as media_consumo_km_l,\n",
    "\tround(sum(dist_km::numeric),2) as dist_km, \n",
    "\tround(sum((dist_km::numeric / media_consumo_km_l::numeric) * (select AVG(preco_medio_revenda::NUMERIC) from preco_combustivel_semanal where estado = 'RIO DE JANEIRO' and produto like '%GAS%')),2) as custo_viagem  \n",
    "from veiculos_clusterizados\n",
    "where cluster_consumo = '2'\n",
    "group by nome_usuario,\n",
    "\tmedia_consumo_km_l\t\n",
    "order by media_consumo_km_l::numeric desc\n",
    "limit 5\n",
    "\"\"\"\n",
    "cursor.execute(melhor_desempenho)\n",
    "resultados_melhor_desempenho = cursor.fetchall()\n",
    "conn.close()\n",
    "df_melhor_desempenho = pd.DataFrame(resultados_melhor_desempenho, columns=[desc[0] for desc in cursor.description])\n",
    "df_melhor_desempenho = df_melhor_desempenho.sort_values(by='custo_viagem')\n",
    "\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "preco_medio_gasolina = \"\"\"\n",
    "select \n",
    "\tavg(preco_medio_revenda::numeric) as media\n",
    "from preco_combustivel_semanal\n",
    "where\n",
    "\t produto like '%GASOLINA%' and \n",
    "     estado = 'RIO DE JANEIRO'\n",
    "\"\"\"\n",
    "cursor.execute(preco_medio_gasolina)\n",
    "resultados_preco_medio_gasolina = cursor.fetchall()\n",
    "conn.close()\n",
    "df_preco_medio_gasolina = pd.DataFrame(resultados_preco_medio_gasolina, columns=[desc[0] for desc in cursor.description])\n",
    "valor_medio = Decimal(df_preco_medio_gasolina['media'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pior_desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melhor_desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "valor_medio = Decimal(df_preco_medio_gasolina['media'].iloc[0])\n",
    "valor_medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulado = pd.DataFrame({\n",
    "    'nome_usuario_original': df_pior_desempenho['nome_usuario'],\n",
    "    'media_consumo_km_original': df_pior_desempenho['media_consumo_km_l'],\n",
    "    'nome_usuario_simulado': df_melhor_desempenho['nome_usuario'],\n",
    "    'media_consumo_km_l_simulado': df_melhor_desempenho['media_consumo_km_l'],\n",
    "    'dist_km_original': df_pior_desempenho['dist_km'],\n",
    "    'custo_viagem_original': df_pior_desempenho['custo_viagem'],    \n",
    "    'custo_viagem_simulado': df_pior_desempenho['dist_km'] / df_melhor_desempenho['media_consumo_km_l'] * valor_medio\n",
    "    })\n",
    "df_simulado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converta o DataFrame enriquecido de volta para CSV\n",
    "csv_simulado = df_simulado.to_csv(index=False, sep=';')\n",
    "csv_simulado_bytes = csv_simulado.encode('utf-8')\n",
    "\n",
    "## Crie um buffer de bytes\n",
    "csv_simulado_buffer = BytesIO(csv_simulado_bytes)\n",
    "nome_arquivo = f'simulacao_rota_veiculos_{data_hora_atual}.csv'\n",
    "\n",
    "minioclient.put_object(\n",
    "        CLUSTERING,\n",
    "        nome_arquivo,  # Use o mesmo nome de arquivo\n",
    "        data=csv_simulado_buffer,\n",
    "        length=len(csv_simulado_bytes),\n",
    "        content_type='application/csv')\n",
    "\n",
    "## Enviando a simulação para o Banco \n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "truncate = \"\"\" truncate table cluster_simulacao; \"\"\"\n",
    "cursor.execute(truncate)\n",
    "\n",
    "copy_sql = \"\"\"\n",
    "    COPY cluster_simulacao (nome_usuario_original,media_consumo_km_original,nome_usuario_simulado,media_consumo_km_l_simulado,dist_km_original,custo_viagem_original,custo_viagem_simulado)\n",
    "    FROM stdin WITH CSV HEADER DELIMITER as ';'\n",
    "    \"\"\"\n",
    "\n",
    "arquivos_clustering = [arquivo for arquivo in minioclient.list_objects(CLUSTERING) if arquivo.object_name.endswith(\".csv\")]\n",
    "## Itera sobre cada arquivo CSV encontrado no Minio\n",
    "for arquivo_clustering in arquivos_clustering:\n",
    "    # Obtém o objeto do arquivo CSV do Minio  \n",
    "    obj_regr = minioclient.get_object(CLUSTERING, arquivo_clustering.object_name)\n",
    "    csv_decod = obj_regr.data.decode('utf-8')  # Convertendo bytes para string\n",
    "    arquivo_csv = StringIO(csv_decod)\n",
    "    df = pd.read_csv(arquivo_csv, sep=';')\n",
    "    csv_bytes = df.to_csv(index=False,sep=';').encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "    nome_arquivo = arquivo_clustering.object_name\n",
    "\n",
    "    with io.StringIO(csv_decod) as file:\n",
    "        cursor.copy_expert(sql=copy_sql, file=file)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos_clustering = [arquivo for arquivo in minioclient.list_objects(CLUSTERING) if arquivo.object_name.endswith(\".csv\")]\n",
    "## Itera sobre cada arquivo CSV encontrado no Minio\n",
    "for arquivo_clustering in arquivos_clustering:\n",
    "    # Obtém o objeto do arquivo CSV do Minio      \n",
    "    nome_arquivo = arquivo_clustering.object_name    \n",
    "    minioclient.remove_object(CLUSTERING, nome_arquivo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minioclient.put_object(\n",
    "        ML_RESULTS,\n",
    "        arquivo_clustering.object_name,  # Use o mesmo nome de arquivo\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv')\n",
    "\n",
    "minioclient.remove_object(CLUSTERING, nome_arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova versão da distancia de Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from minio import Minio\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "data_hora_atual = datetime.now()\n",
    "from time import sleep\n",
    "from decimal import Decimal\n",
    "from scipy.spatial.distance import  minkowski\n",
    "\n",
    "\n",
    "minioclient = Minio('localhost:9000',\n",
    "    access_key='minioadmin',\n",
    "    secret_key='minioadmin',\n",
    "    secure=False)\n",
    "\n",
    "\n",
    "db_config = {\n",
    "'host': 'localhost',\n",
    "'database': 'postgres',\n",
    "'user': 'postgres',\n",
    "'password': 'postgres',\n",
    "}\n",
    "\n",
    "# Cálculo da distância Euclidiana\n",
    "def calc_dist_euclidiana(row,p=2):\n",
    "    coord1 = (float(row['first_long']), float(row['first_lat']))\n",
    "    coord2 = (float(row['last_long']), float(row['last_lat']))\n",
    "    return minkowski(coord1, coord2, p)\n",
    "\n",
    "# Cálculo da distância de Manhattan\n",
    "def calc_dist_manhattan(row, p=1):\n",
    "    coord1 = (float(row['first_long']), float(row['first_lat']))\n",
    "    coord2 = (float(row['last_long']), float(row['last_lat']))\n",
    "    return minkowski(coord1, coord2, p)\n",
    "\n",
    "# Cálculo da distância de Minkowski\n",
    "def calc_dist_minkowski(row, p=3):\n",
    "    coord1 = (float(row['first_long']), float(row['first_lat']))\n",
    "    coord2 = (float(row['last_long']), float(row['last_lat']))\n",
    "    return minkowski(coord1, coord2, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "with euclidean_manhattan_minkowski as (\n",
    "\tSELECT \n",
    "\t\tdistinct \t\n",
    "\t\tid_rota,\n",
    "\t\tnome_usuario,\n",
    "\t\tdata_rota, \n",
    "\t\tcidade,\n",
    "\t\testado,\n",
    "\t\tpais,\n",
    "\t\tfirst_value(hora_rota) OVER (PARTITION BY id_rota ORDER BY id_rota)::time without time zone AS inicio_rota,\n",
    "\t\tlast_value(hora_rota) OVER (PARTITION BY id_rota ORDER BY id_rota)::time without time zone AS fim_rota,\n",
    "\t\tfirst_value(longitude) OVER (PARTITION BY id_rota ORDER BY id_rota) AS first_long,\n",
    "\t\tlast_value(longitude) OVER (PARTITION BY id_rota ORDER BY id_rota) AS last_long,\n",
    "\t\tfirst_value(latitude) OVER (PARTITION BY id_rota ORDER BY id_rota) AS first_lat,\n",
    "\t\tlast_value(latitude) OVER (PARTITION BY id_rota ORDER BY id_rota) AS last_lat\n",
    "FROM tb_gpx_full)\n",
    "\n",
    "SELECT\n",
    "\tid_rota,\n",
    "    nome_usuario,\n",
    "    data_rota,\n",
    "    inicio_rota,\n",
    "    fim_rota,\n",
    "\tcase \n",
    "\t\twhen fim_rota > inicio_rota then fim_rota - inicio_rota\n",
    "        when inicio_rota > fim_rota then inicio_rota - fim_rota \n",
    "    end as tempo_viagem,\n",
    "    cidade,\n",
    "    estado,\n",
    "    pais,\n",
    "    first_long,\n",
    "\tlast_long,\n",
    "\tfirst_lat,\n",
    "\tlast_lat\n",
    "FROM euclidean_manhattan_minkowski\n",
    "\"\"\"\n",
    "\n",
    "conn = psycopg2.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql_query)\n",
    "pre_manhattan = cursor.fetchall()\n",
    "conn.close()\n",
    "df_pre_manhattan = pd.DataFrame(pre_manhattan, columns=[desc[0] for desc in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_manhattan.head()\n",
    "#print(df_pre_manhattan.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_manhattan[['first_long', 'first_lat', 'last_long', 'last_lat']] = df_pre_manhattan[['first_long', 'first_lat', 'last_long', 'last_lat']].astype(float).round(10)\n",
    "\n",
    "df_pre_manhattan['dist_euclidiana'] = df_pre_manhattan.apply(calc_dist_euclidiana, axis=1)\n",
    "df_pre_manhattan['dist_manhattan'] = df_pre_manhattan.apply(calc_dist_manhattan, axis=1)\n",
    "df_pre_manhattan['dist_minkowski'] = df_pre_manhattan.apply(lambda row: calc_dist_minkowski(row, p=3), axis=1)  \n",
    "\n",
    "df_pre_manhattan['dist_euclidiana'] *= 1000\n",
    "df_pre_manhattan['dist_manhattan'] *= 1000\n",
    "df_pre_manhattan['dist_minkowski'] *= 1000\n",
    "\n",
    "df_euclidean_manhattan_minkowski = df_pre_manhattan\n",
    "df_euclidean_manhattan_minkowski.head()\n",
    "\n",
    "\n",
    "\n",
    "df_filtrado = df_euclidean_manhattan_minkowski[\n",
    "    (df_euclidean_manhattan_minkowski['data_rota'].notnull()) &\n",
    "    (df_euclidean_manhattan_minkowski['inicio_rota'].notnull()) &\n",
    "    (df_euclidean_manhattan_minkowski['id_rota'] == '11189161') &\n",
    "    (df_euclidean_manhattan_minkowski['nome_usuario'] == 'FORD_MUSTANG')\n",
    "]\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euclidean_manhattan_minkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_manhattan.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('11190523__routes__HYUNDAI_IONIQ_5_2022.csv', sep=';')\n",
    "df['lat1'] = df['latitude'].shift()\n",
    "df['lon1'] = df['longitude'].shift()\n",
    "df = df.rename(columns={'latitude': 'lat2', 'longitude': 'lon2'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters Identificados:\n",
    "\n",
    "#### Cada linha do DataFrame agora está associada a um cluster (grupo identificado pelo K-means). Esses clusters representam grupos de viagens com padrões semelhantes de consumo de combustível e distância percorrida.\n",
    "\n",
    "#### Comparação entre Veículos:\n",
    "#### Ao analisar o DataFrame agrupado, você pode identificar quais veículos ou usuários estão no mesmo cluster. Isso indica que esses veículos têm comportamentos semelhantes em termos de eficiência de combustível e distância percorrida.\n",
    "\n",
    "#### Eficiência de Combustível e Distância:\n",
    "#### Dentro de cada cluster, observe a variação nos valores de dist_km e media_consumo_km_l. Isso ajuda a entender como diferentes veículos se comportam em termos de eficiência de combustível para distâncias específicas.\n",
    "\n",
    "#### Detecção de Padrões Anômalos:\n",
    "#### Viagens que não seguem o padrão esperado podem ser identificadas. Por exemplo, se a maioria dos veículos em um cluster possui alta eficiência de combustível para uma determinada distância, viagens que se desviam desse padrão podem ser investigadas.\n",
    "\n",
    "#### Otimização de Rotas e Consumo:\n",
    "#### Se você tiver informações adicionais sobre as rotas (por exemplo, cidades, estradas, condições de tráfego), pode-se explorar se o clusterização reflete padrões relacionados a esses fatores, o que poderia ajudar na otimização de rotas e consumo.\n",
    "\n",
    "#### Exemplos Claros:\n",
    "#### Cluster 1: Eficientes em Cidade:\n",
    "#### Veículos neste cluster podem ter alta eficiência de combustível em distâncias curtas, indicando eficiência em ambientes urbanos.\n",
    "\n",
    "#### Cluster 2: Viagens de Longa Distância:\n",
    "#### Este cluster pode conter veículos que são eficientes em viagens de longa distância, indicando bom desempenho em rodovias.\n",
    "\n",
    "#### Cluster 3: Comportamento Variado:\n",
    "#### Viagens neste cluster podem ter uma combinação de eficiência de combustível e distâncias, representando comportamentos variados.\n",
    "\n",
    "#### Justificativa para Utilização:\n",
    "#### Personalização de Estratégias:\n",
    "#### Compreender esses padrões permite personalizar estratégias de otimização, como fornecer recomendações específicas para motoristas ou ajustar políticas de manutenção.\n",
    "\n",
    "#### Identificação de Tendências de Consumo:\n",
    "#### Ajuda a identificar tendências gerais no consumo de combustível, proporcionando insights valiosos para tomada de decisões informadas.\n",
    "\n",
    "#### Economia de Recursos:\n",
    "#### Pode levar a economias significativas otimizando rotas, manutenção preventiva e promoção de práticas eficientes.\n",
    "#### Ao interpretar os resultados, é essencial considerar o contexto específico do seu conjunto de dados e os objetivos do seu negócio. A análise pode ser refinada com mais detalhes conforme você explora mais características e contextos específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_agrup_veiculos[['dist_km', 'tipo_combustivel', 'custo_trajeto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "type(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = scaler.fit_transform(X)\n",
    "type(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X_normalized)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrup_veiculos['nome_usuario'] = kmeans.labels_\n",
    "df_agrup_veiculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_agrup_veiculos[['nome_usuario','dist_km', 'tipo_combustivel', 'custo_trajeto']]\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(X_normalized)\n",
    "df_agrup_veiculos['inicio_viagem'] = kmeans.labels_\n",
    "df_agrup_veiculos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agrup_veiculos.groupby('inicio_viagem').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_agrup_veiculos, x='nome_usuario', y='custo_trajeto', hue='inicio_viagem', palette='viridis')\n",
    "plt.title('Consumo de Combustível por Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecionando as features relevantes\n",
    "X = df_agrup_veiculos[['dist_km', 'media_consumo_km_l']]\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Especificando o número de clusters (você pode ajustar conforme necessário)\n",
    "num_clusters = 3\n",
    "\n",
    "# Aplicando o K-means\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "df_agrup_veiculos['cluster_consumo'] = kmeans.fit_predict(X_normalized)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(df_agrup_veiculos[['dist_km', 'media_consumo_km_l', 'cluster_consumo']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a média do consumo de combustível para cada cluster\n",
    "media_consumo_por_cluster = df_agrup_veiculos.groupby('cluster_consumo')['media_consumo_km_l'].mean()\n",
    "\n",
    "# Identificando os veículos que estão acima da média em cada cluster\n",
    "veiculos_mais_consumo = {}\n",
    "for cluster, media_consumo in media_consumo_por_cluster.items():\n",
    "    veiculos_cluster = df_agrup_veiculos[df_agrup_veiculos['cluster_consumo'] == cluster]\n",
    "    veiculos_acima_media = veiculos_cluster[veiculos_cluster['media_consumo_km_l'] > media_consumo]\n",
    "    veiculos_mais_consumo[cluster] = veiculos_acima_media[['nome_usuario', 'media_consumo_km_l']]\n",
    "\n",
    "# Exibindo os resultados\n",
    "for cluster, veiculos in veiculos_mais_consumo.items():\n",
    "    print(f\"\\nCluster {cluster} - Veículos que mais consumiram:\")\n",
    "    print(veiculos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Selecione apenas as colunas necessárias para a análise\n",
    "df_agrup_veiculos = df_agrup_veiculos[['dist_km', 'media_consumo_km_l', 'nome_usuario']].copy()\n",
    "\n",
    "# Remova duplicatas se existirem\n",
    "df_agrup_veiculos.drop_duplicates(subset=['dist_km', 'media_consumo_km_l', 'nome_usuario'], inplace=True)\n",
    "\n",
    "# Normalizar os dados\n",
    "X = df_agrup_veiculos[['dist_km', 'media_consumo_km_l']]\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Especificar o número de clusters\n",
    "num_clusters = 3\n",
    "\n",
    "# Aplicar K-means\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "df_agrup_veiculos['cluster_consumo'] = kmeans.fit_predict(X_normalized)\n",
    "\n",
    "# Calcular a média do consumo de combustível para cada cluster\n",
    "media_consumo_por_cluster = df_agrup_veiculos.groupby('cluster_consumo')['media_consumo_km_l'].mean()\n",
    "\n",
    "# Identificar os veículos que estão acima da média em cada cluster\n",
    "veiculos_mais_consumo = {}\n",
    "for cluster, media_consumo in media_consumo_por_cluster.items():\n",
    "    veiculos_cluster = df_agrup_veiculos[df_agrup_veiculos['cluster_consumo'] == cluster]\n",
    "    veiculos_acima_media = veiculos_cluster[veiculos_cluster['media_consumo_km_l'] > media_consumo]\n",
    "    veiculos_mais_consumo[cluster] = veiculos_acima_media[['nome_usuario', 'media_consumo_km_l']]\n",
    "\n",
    "# Exibir os resultados\n",
    "for cluster, veiculos in veiculos_mais_consumo.items():\n",
    "    print(f\"\\nCluster {cluster} - Veículos que mais consumiram:\")\n",
    "    print(veiculos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizando os clusters\n",
    "sns.scatterplot(data=df_agrup_veiculos, x='dist_km', y='media_consumo_km_l', hue='cluster_consumo', palette='viridis')\n",
    "plt.title('Agrupamento de Viagens Semelhantes')\n",
    "plt.xlabel('Distância (km)')\n",
    "plt.ylabel('Consumo Médio (km/l)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizado não supervisionado com K-means e Voronoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Códigos não mais usados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aqui os arquivos saem da pasta download para a pasta DATALAKE_STAGE\n",
    "\n",
    "# files_to_move = [files for files in os.listdir(DOWNLOADS) if files.endswith(\".gpx\")]\n",
    "# for nome_arquivos in files_to_move:\n",
    "#     origin_path = os.path.join(DOWNLOADS, nome_arquivos)\n",
    "#     destiny_path = os.path.join(DATALAKE_STAGE, nome_arquivos)\n",
    "#     os.rename(origin_path, destiny_path)\n",
    "#     print(f\"Arquivo {nome_arquivos} movido para {destiny_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aqui os arquivos saem da pasta DATALAKE_STAGE para o bucket MinIO\n",
    "\n",
    "# for item in os.listdir(DATALAKE_STAGE):\n",
    "#     caminho_pre_proc = os.path.join(DATALAKE_STAGE, item)\n",
    "#     print(caminho_pre_proc)\n",
    "# \n",
    "#     if os.path.isfile(caminho_pre_proc):\n",
    "#         try:\n",
    "#             minioclient.fput_object(DATA_LAKE, item, caminho_pre_proc)\n",
    "#             print(f\"Arquivo {item} enviado com sucesso para o bucket.\")\n",
    "#         except S3Error as err:\n",
    "#             print(f\"Erro ao enviar o arquivo {item}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DADOS PARA INSERIR NA TABELA DE CONTROLE USANDO O BEAUTIFULSOAP ## #\n",
    "# navegador = requests.get('https://www.openstreetmap.org/user/sunnypilot/traces/9255974')\n",
    "# site = BeautifulSoup(navegador.text, 'html.parser')\n",
    "# data_trace = site.find('div', attrs={'class': 'content-body'})\n",
    "# table_trace = data_trace.find('table')\n",
    "# \n",
    "# filename_row = table_trace.find('th', string='Filename:').parent\n",
    "# owner_row = table_trace.find('th', string='Owner:').parent\n",
    "# uploaded_row = table_trace.find('th', string='Uploaded:').parent\n",
    "# \n",
    "# filename = filename_row.find('td').text.strip().replace('(download)','').strip()\n",
    "# owner = owner_row.find('td').text.strip()\n",
    "# uploaded = uploaded_row.find('td').text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ESTE TRECHO TEM A FINALIDADE DE BAIXAR OS ARQUIVOS ## #\n",
    "#list_rotas_finalizadas\n",
    "#list_rotas_pendentes\n",
    "\n",
    "# //*[@id=\"content\"]/div[2]/div/table/tbody/tr[6]/td - finalizado\n",
    "# //*[@id=\"content\"]/div[2]/div/span = PENDENTE\n",
    "# //*[@id=\"content\"]/div[2]/div/table/tbody/tr[4]/td - pendente\n",
    "\n",
    "# ## Trecho descontinuado -- Quando usa [requests.get] estamos usando o BeautifulSoup - e quando usamos só o [.get] estamos usando o selenium\n",
    "# navegador = webdriver.Chrome(service=servico)\n",
    "# for list_route in lista_rotas:\n",
    "#     sleep(3)\n",
    "#     url = list_route[0]\n",
    "#     navegador.get(url)  #--> Exemplo onde usamos o Selenium somente com o [.get]\n",
    "#     navegador.find_element('xpath','//*[@id=\"content\"]/div[2]/div/table/tbody/tr[1]/td/a').click()\n",
    "# navegador.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Criando as variáveis que serão utilizadas no Spark\n",
    "appname = 'tcc-project'\n",
    "master = 'local'\n",
    "# Criando a sessão Spark\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(appname)\\\n",
    "    .master(master)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a sessão Spark\n",
    "appname = 'tcc-project'\n",
    "master = 'local'\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(appname)\\\n",
    "    .master(master)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Criando as variáveis que serão utilizadas no Spark\n",
    "appname = 'tcc-project'\n",
    "master = 'local'\n",
    "# Criando a sessão Spark\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(appname)\\\n",
    "    .master(master)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n",
    "# Coalesce o DataFrame para um único arquivo\n",
    "df_single_file = df.coalesce(1)\n",
    "# Salve o DataFrame coalesced como um único arquivo CSV\n",
    "df_single_file.write.csv(csv_caminho_arquivo, header=True, mode=\"overwrite\")\n",
    "# Dessa forma o spark gera um csv particionado\n",
    "df.write.csv('pyspark_dataframe.csv', header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
